<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Louis Tiao (Posts about machine learning)</title><link>http://tiao.io/</link><description></description><atom:link href="http://tiao.io/categories/machine-learning.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2018 &lt;a href="mailto:louistiao@me.com"&gt;Louis Tiao&lt;/a&gt; </copyright><lastBuildDate>Sun, 25 Feb 2018 14:24:04 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>A simple way to do Bayesian Deep Learning in Keras</title><link>http://tiao.io/posts/a-simple-way-to-do-bayesian-deep-learning-in-keras/</link><dc:creator>Louis Tiao</dc:creator><description>&lt;div&gt;&lt;div class="section" id="posterior-distribution-over-weights"&gt;
&lt;h2&gt;Posterior distribution over weights&lt;/h2&gt;
&lt;p&gt;We approximate the full posterior distribution over the weights of the layer
using variational inference. We do this by specifying a &lt;em&gt;variational distribution&lt;/em&gt;
which we use to approximate the exact posterior. The variational distribution of
choice here is a simple diagonal Gaussian. The parameters of the variational
distribution are called &lt;em&gt;variational parameters&lt;/em&gt; and consist of the mean and
standard deviation of the layer's weights. They are the parameters of the Keras
layer, which we add with &lt;tt class="docutils literal"&gt;self.add_weight&lt;/tt&gt; (kind of a bad name). We don't take
advantage of the option to add regularizer as we will add the KL directly to the
loss ourselves later.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_6ad72ac267c5493f9379bef924b7f39f-1"&gt;&lt;/a&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kernel_loc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_6ad72ac267c5493f9379bef924b7f39f-2"&gt;&lt;/a&gt;                                  &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'kernel_loc'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_6ad72ac267c5493f9379bef924b7f39f-3"&gt;&lt;/a&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kernel_scale&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_6ad72ac267c5493f9379bef924b7f39f-4"&gt;&lt;/a&gt;                                    &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'kernel_scale'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_6ad72ac267c5493f9379bef924b7f39f-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_6ad72ac267c5493f9379bef924b7f39f-6"&gt;&lt;/a&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;distributions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kernel_loc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_6ad72ac267c5493f9379bef924b7f39f-7"&gt;&lt;/a&gt;                                      &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kernel_scale&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_6ad72ac267c5493f9379bef924b7f39f-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_6ad72ac267c5493f9379bef924b7f39f-9"&gt;&lt;/a&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;use_bias&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_6ad72ac267c5493f9379bef924b7f39f-10"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias_loc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt;
&lt;a name="rest_code_6ad72ac267c5493f9379bef924b7f39f-11"&gt;&lt;/a&gt;                                    &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'bias_loc'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_6ad72ac267c5493f9379bef924b7f39f-12"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias_scale&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt;
&lt;a name="rest_code_6ad72ac267c5493f9379bef924b7f39f-13"&gt;&lt;/a&gt;                                      &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'bias_scale'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_6ad72ac267c5493f9379bef924b7f39f-14"&gt;&lt;/a&gt;
&lt;a name="rest_code_6ad72ac267c5493f9379bef924b7f39f-15"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;distributions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias_loc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_6ad72ac267c5493f9379bef924b7f39f-16"&gt;&lt;/a&gt;                                        &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias_scale&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_6ad72ac267c5493f9379bef924b7f39f-17"&gt;&lt;/a&gt;&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_6ad72ac267c5493f9379bef924b7f39f-18"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="prior-distribution"&gt;
&lt;h2&gt;Prior distribution&lt;/h2&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_510c0bb1250a4aa5a75ede24ea9c59c7-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;kernel_prior&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_510c0bb1250a4aa5a75ede24ea9c59c7-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;kernel_prior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_510c0bb1250a4aa5a75ede24ea9c59c7-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_510c0bb1250a4aa5a75ede24ea9c59c7-4"&gt;&lt;/a&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;bias_prior&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_510c0bb1250a4aa5a75ede24ea9c59c7-5"&gt;&lt;/a&gt;    &lt;span class="n"&gt;bias_prior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_510c0bb1250a4aa5a75ede24ea9c59c7-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_510c0bb1250a4aa5a75ede24ea9c59c7-7"&gt;&lt;/a&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kernel_prior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kernel_prior&lt;/span&gt;
&lt;a name="rest_code_510c0bb1250a4aa5a75ede24ea9c59c7-8"&gt;&lt;/a&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias_prior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bias_prior&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="kl-divergence"&gt;
&lt;h2&gt;KL Divergence&lt;/h2&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_03aa2ffefc97437ea6324dac6e51414c-1"&gt;&lt;/a&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kl_divergence&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kernel_posterior&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_03aa2ffefc97437ea6324dac6e51414c-2"&gt;&lt;/a&gt;                            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kernel_prior&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_03aa2ffefc97437ea6324dac6e51414c-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_03aa2ffefc97437ea6324dac6e51414c-4"&gt;&lt;/a&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kl_divergence&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias_posterior&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_03aa2ffefc97437ea6324dac6e51414c-5"&gt;&lt;/a&gt;                            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias_prior&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;In a &lt;a class="reference external" href="http://tiao.io/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/"&gt;previous post&lt;/a&gt;,
I demonstrated how to leverage Keras' modular design to implement variational
autoencoders in a way that makes it easy to tweak hyperparameters, adapt to it
to other related models, and extend it to the more sophisticated methods
proposed in the current research.&lt;/p&gt;
&lt;p&gt;Recall that we optimize the generally intractable evidence lower bound (ELBO)
using reparameterization gradients, which approximates the expectation of
gradients with Monte Carlo (MC) samples. In their original paper, Kingma and
Welling (2014) &lt;a class="footnote-reference" href="http://tiao.io/posts/a-simple-way-to-do-bayesian-deep-learning-in-keras/#kingma2014" id="id1"&gt;[1]&lt;/a&gt; remark that an MC sample size of 1 is adequate for
a sufficiently large batch size (~100). Obviously, this is highly dependent on
the problem (more specifically the likelihood). In general, it is important to
experiment with different MC sample sizes and observe the various effects it
has on training stability. In this short post, we demonstrate how to tweak the
MC sample size under our basic framework.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://tiao.io/posts/a-simple-way-to-do-bayesian-deep-learning-in-keras/"&gt;Read more…&lt;/a&gt; (5 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><category>bayesian</category><category>deep learning</category><category>keras</category><category>mathjax</category><category>python</category><category>representation learning</category><category>tensorflow</category><category>variational inference</category><guid>http://tiao.io/posts/a-simple-way-to-do-bayesian-deep-learning-in-keras/</guid><pubDate>Sat, 20 Jan 2018 11:47:57 GMT</pubDate></item><item><title>Using negative log-likelihoods of TensorFlow Distributions as Keras losses</title><link>http://tiao.io/posts/using-negative-log-likelihoods-of-tensorflow-distributions-as-keras-losses/</link><dc:creator>Louis Tiao</dc:creator><description>&lt;div&gt;&lt;p&gt;Nothing here yet. This short post is supposed to summarize its
&lt;a class="reference external" href="http://tiao.io/listings/keras/using_negative_log_likelihoods_of_tensorflow_distributions_as_keras_losses.ipynb.html"&gt;supplementary notebook&lt;/a&gt;. For now, please go there instead.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://tiao.io/posts/using-negative-log-likelihoods-of-tensorflow-distributions-as-keras-losses/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>keras</category><category>python</category><category>tensorflow</category><guid>http://tiao.io/posts/using-negative-log-likelihoods-of-tensorflow-distributions-as-keras-losses/</guid><pubDate>Sun, 03 Dec 2017 11:36:10 GMT</pubDate></item></channel></rss>