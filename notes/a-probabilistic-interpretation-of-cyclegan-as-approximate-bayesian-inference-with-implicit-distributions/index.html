<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns#
article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>A Probabilistic Interpretation of CycleGAN as Approximate Bayesian Inference with Implicit Distributions | Louis Tiao</title>
<link href="../../assets/css/all.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/override_nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="http://louistiao.me/notes/a-probabilistic-interpretation-of-cyclegan-as-approximate-bayesian-inference-with-implicit-distributions/">
<link rel="icon" href="../../favicon_16x16.ico" sizes="16x16">
<link rel="icon" href="../../favicon_32x32.ico" sizes="32x32">
<link rel="icon" href="../../favicon_256x256.ico" sizes="256x256">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><!--

    /\\\\\\\\\\\\\\\  /\\\\\\\\\\\     /\\\\\\\\\          /\\\\\              
    \///////\\\/////  \/////\\\///    /\\\\\\\\\\\\\      /\\\///\\\           
           \/\\\           \/\\\      /\\\/////////\\\   /\\\/  \///\\\        
            \/\\\           \/\\\     \/\\\       \/\\\  /\\\      \//\\\      
             \/\\\           \/\\\     \/\\\\\\\\\\\\\\\ \/\\\       \/\\\     
              \/\\\           \/\\\     \/\\\/////////\\\ \//\\\      /\\\     
               \/\\\           \/\\\     \/\\\       \/\\\  \///\\\  /\\\      
                \/\\\        /\\\\\\\\\\\ \/\\\       \/\\\    \///\\\\\/      
                 \///        \///////////  \///        \///       \/////       

--><meta name="author" content="Louis Tiao">
<link rel="prev" href="../../posts/a-probabilistic-interpretation-of-cyclegan-as-approximate-bayesian-inference-with-implicit-distributions/" title="A Probabilistic Interpretation of CycleGAN as Approximate Bayesian Inference with Implicit Distributions" type="text/html">
<meta property="og:site_name" content="Louis Tiao">
<meta property="og:title" content="A Probabilistic Interpretation of CycleGAN as Approximate Bayesian Inf">
<meta property="og:url" content="http://louistiao.me/notes/a-probabilistic-interpretation-of-cyclegan-as-approximate-bayesian-inference-with-implicit-distributions/">
<meta property="og:description" content="Draft
Please do not share or link.

Keras is awesome. It is a very well-designed library that clearly abides by to
its guiding principles of modularity and extensibility and thereby allows us
to easil">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-12-01T13:15:43+11:00">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

  <div class="container">
    <div class="header clearfix">
      <nav><ul class="nav nav-pills pull-right">
<li>
<a href="../../">About</a>
                </li>
<li>
<a href="../../projects/">Projects</a>
                </li>
<li>
<a href="../../posts/">Posts</a>
                </li>
<li>
<a href="../../archive.html">Archive</a>

          
        </li>
</ul></nav><a href="http://louistiao.me/">

        <h3 class="text-muted">
          <span id="blog-title">Louis Tiao</span>
        </h3>
      </a>
    </div>

<!-- TODO Figure out what to do with this stuff -->
<!--     <div class="row">

      <ul class="nav nav-pills pull-right">
    <li>
    <a href="/notes/a-probabilistic-interpretation-of-cyclegan-as-approximate-bayesian-inference-with-implicit-distributions/index.rst" id="sourcelink">Source</a>
    </li>
          
      </ul>
    </div> -->

    <div id="content" role="main">
      <div class="body-content">
        <!--Body content-->
        <div class="row">
          
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">A Probabilistic Interpretation of CycleGAN as Approximate Bayesian Inference with Implicit Distributions</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    Louis Tiao
            </span></p>
            <p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2017-12-01T13:15:43+11:00" itemprop="datePublished" title="2017-12-01 13:15">2017-12-01 13:15</time></a></p>
                <p class="commentline">            <a href="#disqus_thread" data-disqus-identifier="cache/content/notes/a-probabilistic-interpretation-of-cyclegan-as-approximate-bayesian-inference-with-implicit-distributions.html">Comments</a>


                    </p>
<p class="sourceline"><a href="index.rst" class="sourcelink">Source</a></p>

        </div>
        
    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<!-- title: Implementing Variational Autoencoders in Keras: Beyond the Quickstart Tutorial -->
<!-- slug: implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial -->
<!-- date: 2017-10-23 01:19:59 UTC+11:00 -->
<!-- tags: variational inference, keras, tensorflow, python, variational autoencoder, unsupervised learning, deep learning, representation learning, mathjax -->
<!-- category: coding -->
<!-- link: -->
<!-- description: -->
<!-- type: text -->
<div class="admonition admonition-draft">
<p class="first admonition-title">Draft</p>
<p class="last">Please do not share or link.</p>
</div>
<p><a class="reference external" href="https://keras.io/">Keras</a> is awesome. It is a very well-designed library that clearly abides by to
its <a class="reference external" href="https://keras.io/#guiding-principles">guiding principles</a> of modularity and extensibility and thereby allows us
to easily assemble powerful complex models from primitive building blocks.
This has been demonstrated by many blog posts and tutorials, such as the
excellent tutorial on <a class="reference external" href="https://blog.keras.io/building-autoencoders-in-keras.html">Building Autoencoders in Keras</a>.
As the name suggests, that tutorial provides examples of how to implement
various kinds of autoencoders in Keras, including the variational autoencoder
(VAE) <a class="footnote-reference" href="#kingma2014" id="id1">[1]</a>.</p>
<div class="figure align-center">
<img alt="../../images/vae/result_combined.png" src="../../images/vae/result_combined.png"><p class="caption">Visualization of 2D manifold of MNIST digits (left)
and the representation of digits in latent space colored according to their
digit labels (right).</p>
</div>
<p>Like all autoencoders, the variational autoencoder are primarily used for
unsupervised learning of hidden representations.
However, variational autoencoders are fundamentally different to your standard
neural network-based autoencoder in that they tackle the problem with a
probabilistic approach: by specifying distributions over the observed and
latent variables, and approximating the intractable posterior over the latter
using variational inference with an <em>inference network</em>
<a class="footnote-reference" href="#inference1" id="id2">[2]</a> <a class="footnote-reference" href="#inference2" id="id3">[3]</a>.</p>
<!-- TEASER_END -->
<p>While the examples in the aforementioned tutorial do well to showcase the
versatility of Keras on a wide range of autoencoder model architectures,
<a class="reference external" href="https://github.com/fchollet/keras/blob/2.0.8/examples/variational_autoencoder.py">its implementation of the variational autoencoder</a> doesn't properly take
advantage of Keras' modular design, making it difficult to generalize and
extend in important ways. As we will see, it relies on implementing custom
layers and constructs that are restricted to a specific instance of
variational autoencoders. This is a shame, because when combined, Keras'
building blocks are powerful enough to encapsulate most variants of the
variational autoencoder and more generally, a large family of
<em>deep latent Gaussian models</em> combined with inference networks.</p>
<p>The goal of this post is to propose a clean and elegant alternative
implementation that takes better advantage of Keras' modular design.
It is not a tutorial on variational autoencoders <a class="footnote-reference" href="#id11" id="id4">[*]</a>.
Rather, we study variational autoencoders as a specific case of variational
inference in deep latent Gaussian models with inference networks, and
demonstrate how we can use Keras to implement them in a modular fashion such
that they can be easily adapted to various common problems with different
(non-Gaussian) likelihoods, such as classification with Bayesian logistic /
softmax regression.
This first post will lay the groundwork for a series of future posts that
explore how we can trivially extend this basic modular framework to the more
powerful methods proposed in the latest research, such as the normalizing flows
for building richer posterior approximations <a class="footnote-reference" href="#rezende2015" id="id5">[4]</a>, the Gumbel-softmax
trick for inference in discrete latent variables <a class="footnote-reference" href="#jang2016" id="id6">[5]</a>, and even the most
recent GAN-related density-ratio estimation techniques for likelihood-free
inference <a class="footnote-reference" href="#mescheder2017" id="id7">[6]</a> <a class="footnote-reference" href="#tran2017" id="id8">[7]</a>.</p>
<div class="section" id="model-specification">
<h2>Model specification</h2>
<p>Firstly, it is important to understand that the variational autoencoder
<a class="reference external" href="http://dustintran.com/blog/variational-auto-encoders-do-not-train-complex-generative-models">is not a way to train generative models</a>.
Rather, the generative model is a component of the variational autoencoder and
is in general a deep latent Gaussian model.
Learning in the generative model is done using variational inference, with an
<em>inference network</em> to amortize the cost of inference by sharing statistical
strength and generalization across observed data-points. We first specify the
generative model component, the <em>probabilistic decoder</em>.</p>
<p>latent variables are Gaussian a priori</p>
<div class="section" id="decoder">
<h3>Decoder</h3>
<p>In this example, we let the decoder model
<span class="math">\(p_{\theta}(\mathbf{x}_i | \mathbf{z}_i )\)</span> be a multivariate Bernoulli
whose probabilities are computed from <span class="math">\(\mathbf{z}\)</span> using a fully-connected
neural network with a single hidden layer.</p>
<div class="math">
\begin{align*}
p(\mathbf{z}_i) &amp; = \mathcal{N}(\mathbf{0}, \mathbf{I}), \\
\mathbf{h}_i &amp; = h(\mathbf{W}_1 \mathbf{z}_i + \mathbf{b}_1), \\
p_{\theta}(\mathbf{x}_i | \mathbf{z}_i)
  &amp; = \mathrm{Bern}( \sigma( \mathbf{W}_2 \mathbf{h}_i + \mathbf{b}_2 ) )
\end{align*}
</div>
<pre class="code python"><a name="rest_code_ad846f4081e9408581b4d718d7fa6166-1"></a><span class="n">decoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
<a name="rest_code_ad846f4081e9408581b4d718d7fa6166-2"></a>  <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
<a name="rest_code_ad846f4081e9408581b4d718d7fa6166-3"></a>  <span class="n">Dense</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">)</span>
<a name="rest_code_ad846f4081e9408581b4d718d7fa6166-4"></a><span class="p">])</span>
</pre>
<div class="figure align-center">
<object data="../../images/vae/decoder.svg" style="height: 200px;" type="image/svg+xml">
../../images/vae/decoder.svg</object>
<p class="caption">Decoder architecture.</p>
</div>
<p>By fixing <span class="math">\(\mathbf{W}_1\)</span>, <span class="math">\(\mathbf{b}_1\)</span> and <span class="math">\(h\)</span> to be the
identity matrix, the zero vector, and the identity function respectively
(or equivalently, dropping the first <tt class="docutils literal">Dense</tt> layer in the code snippet above),
we recover <em>logistic factor analysis</em>.
In fact, it is very simple to recover members from the large family of deep
latent Gaussian models, which includes <em>non-linear factor analysis</em>,
<em>non-linear Gaussian belief networks</em>, <em>sigmoid belief networks</em>, and many others.</p>
<p>We could, for example, adapt this to solve multi-class classification with
Bayesian softmax regression by swapping the final layer for</p>
<pre class="code python"><a name="rest_code_4330a76004fb414986a4458c5f8bdc9a-1"></a><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)</span>
</pre>
<p>with amortized variational inference</p>
<p>Bayesian modelling assumes observed variables are fully governed by latent
variables and related through the likelihood / generative model.</p>
<p>Intractable, resort to variational inference.</p>
<p>When combined end-to-end, the inference network and the deep latent Gaussian
model can be seen as having an autoencoder structure.
Indeed, this general structure contains the variational autoencoder as a special
case, and more traditionally, the Helmholtz machine.
Even more generally, we can use this structure to perform amortized variational
inference in complex generative models for a wide array of supervised,
unsupervised and semi-supervised tasks.</p>
<p>The loss we wish to minimize is the <em>negative</em> of the <em>evidence lower bound</em>
(ELBO), which is expressed as</p>
<div class="math">
\begin{align*}
\mathrm{ELBO}(q)
&amp;=
\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})} [
  \log p_{\theta}(\mathbf{x} | \mathbf{z}) +
  \log p(\mathbf{z}) -
  \log q_{\phi}(\mathbf{z} | \mathbf{x})
] \\
&amp;=
\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})} [
  \log p_{\theta}(\mathbf{x} | \mathbf{z})
] - \mathrm{KL} [q_{\phi}(\mathbf{z} | \mathbf{x}) \| p(\mathbf{z}) ].
\end{align*}
</div>
<ul class="simple">
<li>minimizes the KL</li>
<li>approximately maximizes the log marginal likelihood</li>
</ul>
</div>
<div class="section" id="encoder">
<h3>Encoder</h3>
<p>In the specific case of autoencoders, the network that maps latent code</p>
<p>More the general case of amortized variational inference, this is known as a
recognition model, or an inference network.</p>
<div class="math">
\begin{equation*}
q_{\phi}(\mathbf{z} | \mathbf{x})
=
\mathcal{N}(
  \mathbf{z} |
  \mathbf{\mu}_{\phi}(\mathbf{x}),
  \mathrm{diag}(\mathbf{\sigma}_{\phi}^2(\mathbf{x}))
)
\end{equation*}
</div>
<div class="section" id="reparameterization-using-keras-layers">
<h4>Reparameterization using Keras Layers</h4>
<div class="math">
\begin{align*}
\nabla_{\phi}
\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})} [ f(\mathbf{x}, \mathbf{z}) ]
&amp;= \nabla_{\phi} \mathbb{E}_{p(\mathbf{\epsilon})} [
   f(\mathbf{x},
     g_{\phi}(\mathbf{x}, \mathbf{\epsilon}))
] \\
&amp;= \mathbb{E}_{p(\mathbf{\epsilon})} [
 \nabla_{\phi}
 f(\mathbf{x},
   g_{\phi}(\mathbf{x}, \mathbf{\epsilon}))
] \\
\end{align*}
</div>
<p>Specifying <span class="math">\(f(\mathbf{x}, \mathbf{z}) = \log p_{\theta}(\mathbf{x} , \mathbf{z}) - \log q_{\phi}(\mathbf{z} | \mathbf{x})\)</span> gives us the gradient of the ELBO above.</p>
<div class="math">
\begin{equation*}
z = g_{\phi}(\mathbf{x}, \mathbf{\epsilon}), \quad
  \mathbf{\epsilon} \sim p(\mathbf{\epsilon})
\end{equation*}
</div>
<div class="math">
\begin{equation*}
g_{\phi}(\mathbf{x}, \mathbf{\epsilon}) =
  \mathbf{\mu}_{\phi}(\mathbf{x}) +
  \mathbf{\sigma}_{\phi}(\mathbf{x}) \odot
  \mathbf{\epsilon}, \quad
  \mathbf{\epsilon} \sim
  \mathcal{N}(\mathbf{0}, \mathbf{I})
\end{equation*}
</div>
<p>Assume <tt class="docutils literal">z_mu</tt> and <tt class="docutils literal">z_sigma</tt> are the outputs of some layers. Then, using
<a class="reference external" href="https://keras.io/layers/merge/">Merge Layers</a>, <tt class="docutils literal">Add</tt> and <tt class="docutils literal">Multiply</tt>:</p>
<pre class="code python"><a name="rest_code_2fd69742d63f4af69bfc9e50888d0ba8-1"></a><span class="n">eps</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,))</span>
<a name="rest_code_2fd69742d63f4af69bfc9e50888d0ba8-2"></a><span class="n">z_eps</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">()([</span><span class="n">z_sigma</span><span class="p">,</span> <span class="n">eps</span><span class="p">])</span>
<a name="rest_code_2fd69742d63f4af69bfc9e50888d0ba8-3"></a>
<a name="rest_code_2fd69742d63f4af69bfc9e50888d0ba8-4"></a><span class="n">z</span> <span class="o">=</span> <span class="n">Add</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_eps</span><span class="p">])</span>
</pre>
<div class="figure align-center">
<object data="../../images/vae/reparameterization.svg" style="height: 300px;" type="image/svg+xml">
../../images/vae/reparameterization.svg</object>
<p class="caption">Reparameterization with simple location-scale transformation using Keras
merge layers.</p>
</div>
<p>Lambda layer, which simultaneously draws samples from a hard-coded base
distribution and performs reparameterization. This implementation achieves a
more appropriate level of modularity and abstraction. It's makes it clear that
each of these atomic building blocks are themselves deterministic
transformations which together make up a deterministic transformation.
The source of stochasticity comes from the input, which we are able to tweak at
test time. Gumbel-softmax trick.</p>
<pre class="code python"><a name="rest_code_5cc331e7229f437983a29fa4b188b4e9-1"></a><span class="n">eps</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">K</span><span class="o">.</span><span class="n">random_normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">latent_dim</span><span class="p">)))</span>
</pre>
<p>For the sake of illustration, we've fixed <tt class="docutils literal">sigma</tt> and <tt class="docutils literal">mu</tt> as <tt class="docutils literal">Input</tt>
layers. That's why it says <tt class="docutils literal">InputLayer</tt> next to it. In reality, it will be
the output layer of a network. We specify <span class="math">\(\mathbf{\mu}_{\phi}(\mathbf{x})\)</span>
and <span class="math">\(\mathbf{\sigma}_{\phi}(\mathbf{x})\)</span> now.</p>
<pre class="code python"><a name="rest_code_a989f3efcba04c58a2d68b4a21d07493-1"></a><span class="n">x</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,))</span>
<a name="rest_code_a989f3efcba04c58a2d68b4a21d07493-2"></a><span class="n">h</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<a name="rest_code_a989f3efcba04c58a2d68b4a21d07493-3"></a>
<a name="rest_code_a989f3efcba04c58a2d68b4a21d07493-4"></a><span class="n">z_mu</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_a989f3efcba04c58a2d68b4a21d07493-5"></a><span class="n">z_log_var</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_a989f3efcba04c58a2d68b4a21d07493-6"></a><span class="n">z_sigma</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="o">*</span><span class="n">t</span><span class="p">))(</span><span class="n">z_log_var</span><span class="p">)</span>
</pre>
<div class="figure align-center">
<object data="../../images/vae/encoder.svg" style="height: 500px;" type="image/svg+xml">
../../images/vae/encoder.svg</object>
<p class="caption">Encoder architecture.</p>
</div>
</div>
<div class="section" id="kl-divergence">
<h4>KL Divergence</h4>
<p>We choose prior <span class="math">\(p(\mathbf{z})\)</span> to be</p>
<div class="math">
\begin{equation*}
p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I}).
\end{equation*}
</div>
<p>latent space regularization</p>
<div class="math">
\begin{equation*}
\mathrm{KL} [q_{\phi}(\mathbf{z} | \mathbf{x}) \| p(\mathbf{z}) ]
= - \frac{1}{2} \sum_{k=1}^K \{ 1 + \log \sigma_k^2 - \mu_k^2 - \sigma_k^2 \}
\end{equation*}
</div>
<pre class="code python"><a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-1"></a><span class="k">class</span> <span class="nc">KLDivergenceLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-2"></a>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-3"></a>    <span class="sd">""" Identity transform layer that adds KL divergence</span>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-4"></a><span class="sd">    to the final model loss.</span>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-5"></a><span class="sd">    """</span>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-6"></a>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-7"></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-8"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">is_placeholder</span> <span class="o">=</span> <span class="bp">True</span>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-9"></a>        <span class="nb">super</span><span class="p">(</span><span class="n">KLDivergenceLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-10"></a>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-11"></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-12"></a>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-13"></a>        <span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span> <span class="o">=</span> <span class="n">inputs</span>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-14"></a>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-15"></a>        <span class="n">kl_batch</span> <span class="o">=</span> <span class="o">-</span> <span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">log_var</span> <span class="o">-</span>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-16"></a>                                <span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span> <span class="o">-</span>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-17"></a>                                <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_var</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-18"></a>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-19"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">kl_batch</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-20"></a>
<a name="rest_code_b53539ecea0f494db2e6e5bcd66d2b47-21"></a>        <span class="k">return</span> <span class="n">inputs</span>
</pre>
<pre class="code python"><a name="rest_code_c090837f486e48b38e562deabb50c51e-1"></a><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">KLDivergenceLayer</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">])</span>
</pre>
<p>by itself, it will learn to ignore the input and map all outputs to 0.
It is only when we tack on the decoder that the reconstruction likelihood
is introduced. Only then will we reconcile the likelihood / observed data with
our prior to form the posterior over latent codes.</p>
<p>At this stage we could specify
<tt class="docutils literal">prob_encoder = Model(inputs=x, <span class="pre">outputs=[z_mu,</span> z_sigma])</tt>
and compile it with something like
<tt class="docutils literal"><span class="pre">prob_encoder.compile(optimizer='rmsprop`,</span> loss=None)</tt>.
When we fit it, it would trivially map all inputs to 0 and 1, thus learning the
prior distribution.</p>
<p>inputs mu and log_var are of shape (batch_size, latent_dim)
the loss we add should be scalar. this is unlike loss
function specified in model compile which should returns
loss vector of shape (batch_size,) since it requires
loss for each datapoint in the batch for sample
weighting.</p>
<div class="figure align-center">
<object data="../../images/vae/encoder_full.svg" style="height: 500px;" type="image/svg+xml">
../../images/vae/encoder_full.svg</object>
<p class="caption">Full encoder architecture, including auxiliary KL divergence layer.</p>
</div>
</div>
</div>
<div class="section" id="putting-it-all-together">
<h3>Putting it all together</h3>
<pre class="code python"><a name="rest_code_22781621dee14a67bfbf0112c0ada5b4-1"></a><span class="n">x</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,))</span>
<a name="rest_code_22781621dee14a67bfbf0112c0ada5b4-2"></a><span class="n">h</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<a name="rest_code_22781621dee14a67bfbf0112c0ada5b4-3"></a>
<a name="rest_code_22781621dee14a67bfbf0112c0ada5b4-4"></a><span class="n">z_mu</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_22781621dee14a67bfbf0112c0ada5b4-5"></a><span class="n">z_log_var</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_22781621dee14a67bfbf0112c0ada5b4-6"></a>
<a name="rest_code_22781621dee14a67bfbf0112c0ada5b4-7"></a><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">KLDivergenceLayer</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">])</span>
<a name="rest_code_22781621dee14a67bfbf0112c0ada5b4-8"></a><span class="n">z_sigma</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="o">*</span><span class="n">t</span><span class="p">))(</span><span class="n">z_log_var</span><span class="p">)</span>
<a name="rest_code_22781621dee14a67bfbf0112c0ada5b4-9"></a>
<a name="rest_code_22781621dee14a67bfbf0112c0ada5b4-10"></a><span class="n">eps</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">K</span><span class="o">.</span><span class="n">random_normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">latent_dim</span><span class="p">)))</span>
<a name="rest_code_22781621dee14a67bfbf0112c0ada5b4-11"></a><span class="n">z_eps</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">()([</span><span class="n">z_sigma</span><span class="p">,</span> <span class="n">eps</span><span class="p">])</span>
<a name="rest_code_22781621dee14a67bfbf0112c0ada5b4-12"></a><span class="n">z</span> <span class="o">=</span> <span class="n">Add</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_eps</span><span class="p">])</span>
<a name="rest_code_22781621dee14a67bfbf0112c0ada5b4-13"></a>
<a name="rest_code_22781621dee14a67bfbf0112c0ada5b4-14"></a><span class="n">decoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
<a name="rest_code_22781621dee14a67bfbf0112c0ada5b4-15"></a>    <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
<a name="rest_code_22781621dee14a67bfbf0112c0ada5b4-16"></a>    <span class="n">Dense</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">)</span>
<a name="rest_code_22781621dee14a67bfbf0112c0ada5b4-17"></a><span class="p">])</span>
<a name="rest_code_22781621dee14a67bfbf0112c0ada5b4-18"></a>
<a name="rest_code_22781621dee14a67bfbf0112c0ada5b4-19"></a><span class="n">x_mean</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre>
<pre class="code python"><a name="rest_code_55841acd1ecb4e4db56a5d85e698bdf0-1"></a><span class="n">vae</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">eps</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">x_mean</span><span class="p">)</span>
<a name="rest_code_55841acd1ecb4e4db56a5d85e698bdf0-2"></a><span class="n">vae</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'rmsprop'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">nll</span><span class="p">)</span>
</pre>
<div class="figure align-center">
<object data="../../images/vae/vae_full_shapes.svg" style="height: 500px;" type="image/svg+xml">
../../images/vae/vae_full_shapes.svg</object>
<p class="caption">Variational autoencoder architecture.</p>
</div>
<p>The point of this tutorial is to illustrate the general framework for performing
amortized variational inference using Keras, treating the inference network
(approximate posterior) and the generative network (likelihood) as black-boxes.
What we've used for the encoder and decoder each with a single hidden
full-connected layer is perhaps the minimal viable architecture.
In the examples directory, Keras provides a more sophisticated variational
autoencoder with deconvolutional layers. The architecture definitions can be
trivially copy-pasted here without need to modify anything else.</p>
</div>
</div>
<div class="section" id="model-fitting">
<h2>Model fitting</h2>
<p>We load the training data as usual. Now the <tt class="docutils literal">vae</tt> is explicitly specified with
random noise source as an auxiliary input. This allows to easily control the
base distribution <span class="math">\(p(\mathbf{\epsilon})\)</span> and also how we draw Monte Carlo
samples of <span class="math">\(\mathbf{z}\)</span> for each datapoint <span class="math">\(\mathbf{x}\)</span>. Usually
we just stick with a simple isotropic Gaussian distribution and draw a different
MC sample for each datapoint.</p>
<pre class="code python"><a name="rest_code_d5d3b1897294455ca2d6e5186da95b84-1"></a><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<a name="rest_code_d5d3b1897294455ca2d6e5186da95b84-2"></a><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">original_dim</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
<a name="rest_code_d5d3b1897294455ca2d6e5186da95b84-3"></a><span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">original_dim</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
</pre>
<p>Model fitting feels less intuitive. The <tt class="docutils literal">vae</tt> is compiled with <tt class="docutils literal">loss=None</tt>
explicitly specified which raises a warning. When fit is called, the targets
argument is left unspecified, and the reconstruction loss is optimized through
the <cite>CustomLayer</cite>. This mapping from mathematical problem formulation to code
implementation appears more natural and straightforward. It's easy to understand
at a glance from our call to the <tt class="docutils literal">fit</tt> method that we're training a
probabilistic auto-encoder.</p>
<pre class="code python"><a name="rest_code_036b446e29774c9fa832a02481a9b7bf-1"></a><span class="n">vae</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span>
<a name="rest_code_036b446e29774c9fa832a02481a9b7bf-2"></a>        <span class="n">x_train</span><span class="p">,</span>
<a name="rest_code_036b446e29774c9fa832a02481a9b7bf-3"></a>        <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<a name="rest_code_036b446e29774c9fa832a02481a9b7bf-4"></a>        <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
<a name="rest_code_036b446e29774c9fa832a02481a9b7bf-5"></a>        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<a name="rest_code_036b446e29774c9fa832a02481a9b7bf-6"></a>        <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">x_test</span><span class="p">))</span>
</pre>
<p>Personally, I prefer this view since the all sources of stochasticity emanate
from the inputs to the model.</p>
<pre class="code python"><a name="rest_code_b2feceecfa214ec4ad5905da100ab3dc-1"></a><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<a name="rest_code_b2feceecfa214ec4ad5905da100ab3dc-2"></a>
<a name="rest_code_b2feceecfa214ec4ad5905da100ab3dc-3"></a><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">hist</span><span class="o">.</span><span class="n">history</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<a name="rest_code_b2feceecfa214ec4ad5905da100ab3dc-4"></a>
<a name="rest_code_b2feceecfa214ec4ad5905da100ab3dc-5"></a><span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'NELBO'</span><span class="p">)</span>
<a name="rest_code_b2feceecfa214ec4ad5905da100ab3dc-6"></a><span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'# epochs'</span><span class="p">)</span>
<a name="rest_code_b2feceecfa214ec4ad5905da100ab3dc-7"></a>
<a name="rest_code_b2feceecfa214ec4ad5905da100ab3dc-8"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>
<div class="figure align-center">
<object data="../../images/vae/nelbo.svg" style="width: 500px;" type="image/svg+xml">
../../images/vae/nelbo.svg</object>
</div>
<pre class="code python"><a name="rest_code_68a7ef49c3584f24a0ad423f2ecabe79-1"></a><span class="n">encoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z_mu</span><span class="p">)</span>
<a name="rest_code_68a7ef49c3584f24a0ad423f2ecabe79-2"></a>
<a name="rest_code_68a7ef49c3584f24a0ad423f2ecabe79-3"></a><span class="c1"># display a 2D plot of the digit classes in the latent space</span>
<a name="rest_code_68a7ef49c3584f24a0ad423f2ecabe79-4"></a><span class="n">z_test</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<a name="rest_code_68a7ef49c3584f24a0ad423f2ecabe79-5"></a><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<a name="rest_code_68a7ef49c3584f24a0ad423f2ecabe79-6"></a><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">z_test</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z_test</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span>
<a name="rest_code_68a7ef49c3584f24a0ad423f2ecabe79-7"></a>            <span class="n">alpha</span><span class="o">=.</span><span class="mi">4</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">3</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'viridis'</span><span class="p">)</span>
<a name="rest_code_68a7ef49c3584f24a0ad423f2ecabe79-8"></a><span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<a name="rest_code_68a7ef49c3584f24a0ad423f2ecabe79-9"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>
<div class="figure align-center">
<img alt="../../images/vae/result_latent_space.png" src="../../images/vae/result_latent_space.png" style="height: 500px;">
</div>
<pre class="code python"><a name="rest_code_e39e1e4cd7e04c6baaa2e256634f31e6-1"></a><span class="c1"># display a 2D manifold of the digits</span>
<a name="rest_code_e39e1e4cd7e04c6baaa2e256634f31e6-2"></a><span class="n">n</span> <span class="o">=</span> <span class="mi">15</span>  <span class="c1"># figure with 15x15 digits</span>
<a name="rest_code_e39e1e4cd7e04c6baaa2e256634f31e6-3"></a><span class="n">digit_size</span> <span class="o">=</span> <span class="mi">28</span>
<a name="rest_code_e39e1e4cd7e04c6baaa2e256634f31e6-4"></a>
<a name="rest_code_e39e1e4cd7e04c6baaa2e256634f31e6-5"></a><span class="c1"># linearly spaced coordinates on the unit square were transformed</span>
<a name="rest_code_e39e1e4cd7e04c6baaa2e256634f31e6-6"></a><span class="c1"># through the inverse CDF (ppf) of the Gaussian to produce values</span>
<a name="rest_code_e39e1e4cd7e04c6baaa2e256634f31e6-7"></a><span class="c1"># of the latent variables z, since the prior of the latent space</span>
<a name="rest_code_e39e1e4cd7e04c6baaa2e256634f31e6-8"></a><span class="c1"># is Gaussian</span>
<a name="rest_code_e39e1e4cd7e04c6baaa2e256634f31e6-9"></a><span class="n">u_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span>
<a name="rest_code_e39e1e4cd7e04c6baaa2e256634f31e6-10"></a>                               <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">n</span><span class="p">)))</span>
<a name="rest_code_e39e1e4cd7e04c6baaa2e256634f31e6-11"></a><span class="n">z_grid</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">u_grid</span><span class="p">)</span>
<a name="rest_code_e39e1e4cd7e04c6baaa2e256634f31e6-12"></a><span class="n">x_decoded</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">z_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<a name="rest_code_e39e1e4cd7e04c6baaa2e256634f31e6-13"></a><span class="n">x_decoded</span> <span class="o">=</span> <span class="n">x_decoded</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">digit_size</span><span class="p">,</span> <span class="n">digit_size</span><span class="p">)</span>
<a name="rest_code_e39e1e4cd7e04c6baaa2e256634f31e6-14"></a>
<a name="rest_code_e39e1e4cd7e04c6baaa2e256634f31e6-15"></a><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<a name="rest_code_e39e1e4cd7e04c6baaa2e256634f31e6-16"></a><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">x_decoded</span><span class="p">))),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
<a name="rest_code_e39e1e4cd7e04c6baaa2e256634f31e6-17"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>
<div class="figure align-center">
<img alt="../../images/vae/result_manifold.png" src="../../images/vae/result_manifold.png" style="height: 600px;">
</div>
</div>
<div class="section" id="recap">
<h2>Recap</h2>
<ul class="simple">
<li>Demonstration of Sequential and functional Model API</li>
<li>Custom auxiliary layers that augments the model loss</li>
<li>Fixing input to source of stochasticity</li>
<li>Reparameterization using Merge layers</li>
</ul>
</div>
<div class="section" id="what-s-next">
<h2>What's next</h2>
<p>Normalizing flows</p>
<p>We illustrate how to employ the simple Gumbel-Softmax reparameterization to
build a Categorical VAE with discrete latent variables.</p>
<p>We can easily extend <tt class="docutils literal">KLDivergenceLayer</tt> to use an auxiliary density ratio
estimator function, instead of evaluating the KL divergence in the closed-form
expression above.
This relaxes the requirement on approximate posterior
<span class="math">\(q(\mathbf{z}|\mathbf{x})\)</span> (and incidentally, also prior
<span class="math">\(p(\mathbf{z})\)</span>) to yield tractable densities, at the cost of maximizing
a cruder estimate of the ELBO.
This is known as Adversarial Variational Bayes <a class="footnote-reference" href="#mescheder2017" id="id9">[6]</a>, and is an
important line of recent research that extends the applicability of variational
inference to arbitrarily expressive implicit probabilistic models <a class="footnote-reference" href="#tran2017" id="id10">[7]</a>.</p>
</div>
<div class="section" id="footnotes">
<h2>Footnotes</h2>
<table class="docutils footnote" frame="void" id="id11" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id4">[*]</a></td>
<td>
<p class="first">For a complete tutorial on variational autoencoders, I highly recommend:</p>
<ul class="last simple">
<li>
<a class="reference external" href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/">What is a variational autoencoder?</a> by Jaan
Altosaar.</li>
<li>
<a class="reference external" href="https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders</a>
by Carl Doersch.</li>
</ul>
</td>
</tr></tbody>
</table>
</div>
<div class="section" id="references">
<h2>References</h2>
<table class="docutils footnote" frame="void" id="kingma2014" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id1">[1]</a></td>
<td>D. P. Kingma and M. Welling,
"Auto-Encoding Variational Bayes,"
in Proceedings of the 2nd International Conference on Learning
Representations (ICLR), 2014.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="inference1" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id2">[2]</a></td>
<td><a class="reference external" href="http://edwardlib.org/tutorials/inference-networks">Edward tutorial on Inference Networks</a></td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="inference2" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id3">[3]</a></td>
<td>Section "Recognition models and amortised inference" in
<a class="reference external" href="http://blog.shakirm.com/2015/01/variational-inference-tricks-of-the-trade/">Shakir's blog post</a>.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="rezende2015" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id5">[4]</a></td>
<td>D. Rezende and S. Mohamed,
"Variational Inference with Normalizing Flows,"
in Proceedings of the 32nd International Conference on Machine Learning, 2015,
vol. 37, pp. 15301538.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="jang2016" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id6">[5]</a></td>
<td>E. Jang, S. Gu, and B. Poole,
"Categorical Reparameterization with Gumbel-Softmax," Nov. 2016.
in Proceedings of the 5th International Conference on Learning
Representations (ICLR), 2017.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="mescheder2017" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label">[6]</td>
<td>
<em>(<a class="fn-backref" href="#id7">1</a>, <a class="fn-backref" href="#id9">2</a>)</em> L. Mescheder, S. Nowozin, and A. Geiger,
"Adversarial Variational Bayes: Unifying Variational Autoencoders and
Generative Adversarial Networks,"
in Proceedings of the 34th International Conference on Machine Learning, 2017,
vol. 70, pp. 23912400.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="tran2017" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label">[7]</td>
<td>
<em>(<a class="fn-backref" href="#id8">1</a>, <a class="fn-backref" href="#id10">2</a>)</em> D. Tran, R. Ranganath, and D. Blei,
"Hierarchical Implicit Models and Likelihood-Free Variational Inference,"
<em>to appear in</em> Advances in Neural Information Processing Systems 30, 2017.</td>
</tr></tbody>
</table>
</div>
<div class="section" id="appendix">
<h2>Appendix</h2>
<p>Below, you can find:</p>
<ul class="simple">
<li>The <a class="reference external" href="../../listings/vae/variational_autoencoder.ipynb.html">accompanying Jupyter Notebook</a> used to generate the diagrams and plots
in this post.</li>
<li>The above snippets combined in a single executable Python file:</li>
</ul>
<p><a class="reference external" href="../../listings/vae/variational_autoencoder_improved.py.html">vae/variational_autoencoder_improved.py</a>  <a class="reference external" href="../../listings/vae/variational_autoencoder_improved.py">(Source)</a></p>
<pre class="code python"><a name="rest_code_70f0031f36c944d1baead47725a1bd6a-1"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-2"></a><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-3"></a><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-4"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-5"></a><span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-6"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-7"></a><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">,</span> <span class="n">Layer</span><span class="p">,</span> <span class="n">Add</span><span class="p">,</span> <span class="n">Multiply</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-8"></a><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">Sequential</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-9"></a><span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-10"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-11"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-12"></a><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-13"></a><span class="n">original_dim</span> <span class="o">=</span> <span class="mi">784</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-14"></a><span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">2</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-15"></a><span class="n">intermediate_dim</span> <span class="o">=</span> <span class="mi">256</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-16"></a><span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-17"></a><span class="n">epsilon_std</span> <span class="o">=</span> <span class="mf">1.0</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-18"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-19"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-20"></a><span class="k">def</span> <span class="nf">nll</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-21"></a>    <span class="sd">""" Bernoulli negative log likelihood. """</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-22"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-23"></a>    <span class="c1"># keras.losses.binary_crossentropy gives the mean</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-24"></a>    <span class="c1"># over the last axis. We require the sum.</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-25"></a>    <span class="k">return</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">binary_crossentropy</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-26"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-27"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-28"></a><span class="k">class</span> <span class="nc">KLDivergenceLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-29"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-30"></a>    <span class="sd">""" Identity transform layer that adds KL divergence</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-31"></a><span class="sd">    to the final model loss.</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-32"></a><span class="sd">    """</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-33"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-34"></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-35"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">is_placeholder</span> <span class="o">=</span> <span class="bp">True</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-36"></a>        <span class="nb">super</span><span class="p">(</span><span class="n">KLDivergenceLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-37"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-38"></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-39"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-40"></a>        <span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span> <span class="o">=</span> <span class="n">inputs</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-41"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-42"></a>        <span class="n">kl_batch</span> <span class="o">=</span> <span class="o">-</span> <span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">log_var</span> <span class="o">-</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-43"></a>                                <span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span> <span class="o">-</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-44"></a>                                <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_var</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-45"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-46"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">kl_batch</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-47"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-48"></a>        <span class="k">return</span> <span class="n">inputs</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-49"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-50"></a><span class="n">x</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,))</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-51"></a><span class="n">h</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-52"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-53"></a><span class="n">z_mu</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-54"></a><span class="n">z_log_var</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-55"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-56"></a><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">KLDivergenceLayer</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">])</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-57"></a><span class="n">z_sigma</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="o">*</span><span class="n">t</span><span class="p">))(</span><span class="n">z_log_var</span><span class="p">)</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-58"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-59"></a><span class="n">eps</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">K</span><span class="o">.</span><span class="n">random_normal</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="n">epsilon_std</span><span class="p">,</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-60"></a>                                   <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">latent_dim</span><span class="p">)))</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-61"></a><span class="n">z_eps</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">()([</span><span class="n">z_sigma</span><span class="p">,</span> <span class="n">eps</span><span class="p">])</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-62"></a><span class="n">z</span> <span class="o">=</span> <span class="n">Add</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_eps</span><span class="p">])</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-63"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-64"></a><span class="n">decoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-65"></a>    <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-66"></a>    <span class="n">Dense</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">)</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-67"></a><span class="p">])</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-68"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-69"></a><span class="n">x_mean</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-70"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-71"></a><span class="n">vae</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">eps</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">x_mean</span><span class="p">)</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-72"></a><span class="n">vae</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'rmsprop'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">nll</span><span class="p">)</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-73"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-74"></a><span class="c1"># train the VAE on MNIST digits</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-75"></a><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-76"></a><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">original_dim</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-77"></a><span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">original_dim</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-78"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-79"></a><span class="n">vae</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-80"></a>        <span class="n">x_train</span><span class="p">,</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-81"></a>        <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-82"></a>        <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-83"></a>        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-84"></a>        <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">x_test</span><span class="p">))</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-85"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-86"></a><span class="n">encoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z_mu</span><span class="p">)</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-87"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-88"></a><span class="c1"># display a 2D plot of the digit classes in the latent space</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-89"></a><span class="n">z_test</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-90"></a><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-91"></a><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">z_test</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z_test</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-92"></a>            <span class="n">alpha</span><span class="o">=.</span><span class="mi">4</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">3</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'viridis'</span><span class="p">)</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-93"></a><span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-94"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-95"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-96"></a><span class="c1"># display a 2D manifold of the digits</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-97"></a><span class="n">n</span> <span class="o">=</span> <span class="mi">15</span>  <span class="c1"># figure with 15x15 digits</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-98"></a><span class="n">digit_size</span> <span class="o">=</span> <span class="mi">28</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-99"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-100"></a><span class="c1"># linearly spaced coordinates on the unit square were transformed</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-101"></a><span class="c1"># through the inverse CDF (ppf) of the Gaussian to produce values</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-102"></a><span class="c1"># of the latent variables z, since the prior of the latent space</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-103"></a><span class="c1"># is Gaussian</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-104"></a><span class="n">u_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-105"></a>                               <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">n</span><span class="p">)))</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-106"></a><span class="n">z_grid</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">u_grid</span><span class="p">)</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-107"></a><span class="n">x_decoded</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">z_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-108"></a><span class="n">x_decoded</span> <span class="o">=</span> <span class="n">x_decoded</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">digit_size</span><span class="p">,</span> <span class="n">digit_size</span><span class="p">)</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-109"></a>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-110"></a><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-111"></a><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">x_decoded</span><span class="p">))),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
<a name="rest_code_70f0031f36c944d1baead47725a1bd6a-112"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav><ul class="pager hidden-print">
<li class="previous">
                <a href="../../posts/a-probabilistic-interpretation-of-cyclegan-as-approximate-bayesian-inference-with-implicit-distributions/" rel="prev" title="A Probabilistic Interpretation of CycleGAN as Approximate Bayesian Inference with Implicit Distributions">Previous post</a>
            </li>
        </ul></nav></aside><section class="comments hidden-print"><h2>Comments</h2>
                        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="ltiao",
            disqus_url="http://louistiao.me/notes/a-probabilistic-interpretation-of-cyclegan-as-approximate-bayesian-inference-with-implicit-distributions/",
        disqus_title="A Probabilistic Interpretation of CycleGAN as Approximate Bayesian Inference with Implicit Distributions",
        disqus_identifier="cache/content/notes/a-probabilistic-interpretation-of-cyclegan-as-approximate-bayesian-inference-with-implicit-distributions.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section></article><script>var disqus_shortname="ltiao";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>
        <!--End of body content-->
      </div>
    </div>

    <footer id="footer" class="footer">
        
Contents  2017
<a href="mailto:louistiao@me.com">Louis Tiao</a> - Powered by
<a href="https://getnikola.com" rel="nofollow">Nikola</a>


<span class="pull-right">

  <a class="twitter-follow-button" href="https://twitter.com/louistiao" data-show-count="false" data-show-screen-name="false">
  Follow @louistiao
  </a>

  <a class="github-button" href="https://github.com/ltiao" aria-label="Follow @ltiao on GitHub" data-show-count="false">
  Follow @ltiao
  </a>

  <a href="https://ko-fi.com/A3476EX">
    <object type="image/svg+xml" style="pointer-events: none;" data="https://img.shields.io/badge/Support--yellow.svg?style=social"></object>
  </a>

</span>


            
    </footer>
</div> <!-- /container -->

            <script src="../../assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><!-- Google Analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-43722566-1', 'auto');
  ga('send', 'pageview');

</script><!-- GitHub Buttons --><script async defer src="https://buttons.github.io/buttons.js"></script><!-- Twitter Widgets --><script>window.twttr = (function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0],
    t = window.twttr || {};
  if (d.getElementById(id)) return t;
  js = d.createElement(s);
  js.id = id;
  js.src = "https://platform.twitter.com/widgets.js";
  fjs.parentNode.insertBefore(js, fjs);

  t._e = [];
  t.ready = function(f) {
    t._e.push(f);
  };

  return t;
}(document, "script", "twitter-wjs"));</script>
</body>
</html>
