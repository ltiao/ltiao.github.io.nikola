<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns#
article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Implementing Variational Autoencoders in Keras: Beyond the Quickstart Tutorial | Louis Tiao</title>
<link href="../../assets/css/all.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/override_nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/">
<link rel="icon" href="../../favicon_16x16.ico" sizes="16x16">
<link rel="icon" href="../../favicon_32x32.ico" sizes="32x32">
<link rel="icon" href="../../favicon_256x256.ico" sizes="256x256">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><!--

    /\\\\\\\\\\\\\\\  /\\\\\\\\\\\     /\\\\\\\\\          /\\\\\              
    \///////\\\/////  \/////\\\///    /\\\\\\\\\\\\\      /\\\///\\\           
           \/\\\           \/\\\      /\\\/////////\\\   /\\\/  \///\\\        
            \/\\\           \/\\\     \/\\\       \/\\\  /\\\      \//\\\      
             \/\\\           \/\\\     \/\\\\\\\\\\\\\\\ \/\\\       \/\\\     
              \/\\\           \/\\\     \/\\\/////////\\\ \//\\\      /\\\     
               \/\\\           \/\\\     \/\\\       \/\\\  \///\\\  /\\\      
                \/\\\        /\\\\\\\\\\\ \/\\\       \/\\\    \///\\\\\/      
                 \///        \///////////  \///        \///       \/////       

--><meta name="author" content="Louis Tiao">
<meta name="robots" content="noindex">
<meta property="og:site_name" content="Louis Tiao">
<meta property="og:title" content="Implementing Variational Autoencoders in Keras: Beyond the Quickstart ">
<meta property="og:url" content="http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/">
<meta property="og:description" content="Keras is awesome. Its guiding principles ... modularity, extensibility ...
excellent tutorial on Building Autoencoders in Keras
while the example demonstrates the power flexibility of Keras, it fails ">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-10-23T01:19:59+11:00">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="keras">
<meta property="article:tag" content="mathjax">
<meta property="article:tag" content="python">
<meta property="article:tag" content="representation learning">
<meta property="article:tag" content="tensorflow">
<meta property="article:tag" content="unsupervised learning">
<meta property="article:tag" content="variational autoencoder">
<meta property="article:tag" content="variational inference">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

  <div class="container">
    <div class="header clearfix">
      <nav><ul class="nav nav-pills pull-right">
<li>
<a href="../../">About</a>
                </li>
<li>
<a href="../../projects/">Projects</a>
                </li>
<li>
<a href="../">Posts</a>
                </li>
<li>
<a href="../../archive.html">Archive</a>

          
        </li>
</ul></nav><a href="http://louistiao.me/">

        <h3 class="text-muted">
          <span id="blog-title">Louis Tiao</span>
        </h3>
      </a>
    </div>

<!-- TODO Figure out what to do with this stuff -->
<!--     <div class="row">

      <ul class="nav nav-pills pull-right">
    <li>
    <a href="/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/index.rst" id="sourcelink">Source</a>
    </li>
          
      </ul>
    </div> -->

    <div id="content" role="main">
      <div class="body-content">
        <!--Body content-->
        <div class="row">
          
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Implementing Variational Autoencoders in Keras: Beyond the Quickstart Tutorial</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    Louis Tiao
            </span></p>
            <p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2017-10-23T01:19:59+11:00" itemprop="datePublished" title="2017-10-23 01:19">2017-10-23 01:19</time></a></p>
                <p class="commentline">            <a href="#disqus_thread" data-disqus-identifier="cache/content/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial.html">Comments</a>


                    </p>
<p class="sourceline"><a href="index.rst" class="sourcelink">Source</a></p>

        </div>
        
    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p><a class="reference external" href="https://keras.io/">Keras</a> is awesome. Its guiding principles ... modularity, extensibility ...</p>
<p>excellent tutorial on <a class="reference external" href="https://blog.keras.io/building-autoencoders-in-keras.html">Building Autoencoders in Keras</a></p>
<p>while the example demonstrates the power flexibility of Keras, it fails to fully
take advantage of Keras' beautiful design.</p>
<p>The variational autoencoders is currently one of the mainstay generative models.</p>
<p>inference in deep latent Gaussian models (DLGM) with inference networks and
stochastic backpropagation, the combination of which is known as amortized
variational inference. Naturally induces an autoencoder structure.</p>
<p>A number of important shortcomings:</p>
<ul class="simple">
<li>Number of Monte Carlo samples; explicitly as model input</li>
<li>Custom layer vs natural use of primitive / building-blocks</li>
<li>Extensible KL Divergence layer (Adversarial Variational Bayes)</li>
<li>Easy to extend to Normalizing Flows</li>
<li>Natural model of loss / likelihood, easily extends to regression, classification, etc.</li>
<li><a class="reference external" href="https://github.com/fchollet/keras/blob/2.0.8/examples/variational_autoencoder.py">https://github.com/fchollet/keras/blob/2.0.8/examples/variational_autoencoder.py</a></li>
</ul>
<div class="section" id="model-specification">
<h2>Model specification</h2>
<div class="math">
\begin{align*}
\mathrm{ELBO}(q)
&amp;=
\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})} [
  \log p_{\theta}(\mathbf{x} | \mathbf{z}) +
  \log p(\mathbf{z}) -
  \log q_{\phi}(\mathbf{z} | \mathbf{x})
] \\
&amp;=
\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})} [
  \log p_{\theta}(\mathbf{x} | \mathbf{z})
] - \mathrm{KL} [q_{\phi}(\mathbf{z} | \mathbf{x}) \| \log p(\mathbf{z}) ]
\end{align*}
</div>
<div class="section" id="encoder">
<h3>Encoder</h3>
<div class="math">
\begin{equation*}
q_{\phi}(\mathbf{z} | \mathbf{x})
=
\mathcal{N}(
  \mathbf{z} |
  \mathbf{\mu}_{\phi}(\mathbf{x}),
  \mathrm{diag}(\mathbf{\sigma}_{\phi}^2(\mathbf{x}))
)
\end{equation*}
</div>
<div class="section" id="reparameterization-using-keras-layers">
<h4>Reparameterization using Keras Layers</h4>
<div class="math">
\begin{align*}
\nabla_{\phi}
\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})} [ f(\mathbf{x}, \mathbf{z}) ]
&amp;= \nabla_{\phi} \mathbb{E}_{p(\mathbf{\epsilon})} [
   f(\mathbf{x},
     g_{\phi}(\mathbf{x}, \mathbf{\epsilon}))
] \\
&amp;= \mathbb{E}_{p(\mathbf{\epsilon})} [
 \nabla_{\phi}
 f(\mathbf{x},
   g_{\phi}(\mathbf{x}, \mathbf{\epsilon}))
] \\
\end{align*}
</div>
<p>Specifying <span class="math">\(f(\mathbf{x}, \mathbf{z}) = \log p_{\theta}(\mathbf{x} , \mathbf{z}) - \log q_{\phi}(\mathbf{z} | \mathbf{x})\)</span> gives us the gradient of the ELBO above.</p>
<div class="math">
\begin{equation*}
z = g_{\phi}(\mathbf{x}, \mathbf{\epsilon}), \quad
  \mathbf{\epsilon} \sim p(\mathbf{\epsilon})
\end{equation*}
</div>
<div class="math">
\begin{equation*}
g_{\phi}(\mathbf{x}, \mathbf{\epsilon}) =
  \mathbf{\mu}_{\phi}(\mathbf{x}) +
  \mathbf{\sigma}_{\phi}(\mathbf{x}) \odot
  \mathbf{\epsilon}, \quad
  \mathbf{\epsilon} \sim
  \mathcal{N}(\mathbf{0}, \mathbf{I})
\end{equation*}
</div>
<p>Assume <tt class="docutils literal">z_mu</tt> and <tt class="docutils literal">z_sigma</tt> are the outputs of some layers. Then, using
<a class="reference external" href="https://keras.io/layers/merge/">Merge Layers</a>, <tt class="docutils literal">Add</tt> and <tt class="docutils literal">Multiply</tt>:</p>
<pre class="code python"><a name="rest_code_0622eaddf8cd49f7984407e13f87fe80-1"></a><span class="n">eps</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,))</span>
<a name="rest_code_0622eaddf8cd49f7984407e13f87fe80-2"></a><span class="n">z_eps</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">()([</span><span class="n">z_sigma</span><span class="p">,</span> <span class="n">eps</span><span class="p">])</span>
<a name="rest_code_0622eaddf8cd49f7984407e13f87fe80-3"></a>
<a name="rest_code_0622eaddf8cd49f7984407e13f87fe80-4"></a><span class="n">z</span> <span class="o">=</span> <span class="n">Add</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_eps</span><span class="p">])</span>
</pre>
<div class="figure align-center">
<object data="../../images/vae/reparameterization.svg" style="height: 300px;" type="image/svg+xml">
../../images/vae/reparameterization.svg</object>
<p class="caption">Reparameterization with simple location-scale transformation using Keras
merge layers.</p>
</div>
<p>Lambda layer, which simultaneously draws samples from a hard-coded base
distribution and performs reparameterization. This implementation achieves a
more appropriate level of modularity and abstraction. It's makes it clear that
each of these atomic building blocks are themselves deterministic
transformations which together make up a deterministic transformation.
The source of stochasticity comes from the input, which we are able to tweak at
test time. Gumbel-softmax trick.</p>
<p>For the sake of illustration, we've fixed <tt class="docutils literal">sigma</tt> and <tt class="docutils literal">mu</tt> as <tt class="docutils literal">Input</tt>
layers. That's why it says <tt class="docutils literal">InputLayer</tt> next to it. In reality, it will be
the output layer of a network. We specify <span class="math">\(\mathbf{\mu}_{\phi}(\mathbf{x})\)</span>
and <span class="math">\(\mathbf{\sigma}_{\phi}(\mathbf{x})\)</span> now.</p>
<pre class="code python"><a name="rest_code_f6ab3385093946ef8e3f4c3e1b50869d-1"></a><span class="n">x</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,))</span>
<a name="rest_code_f6ab3385093946ef8e3f4c3e1b50869d-2"></a><span class="n">h</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<a name="rest_code_f6ab3385093946ef8e3f4c3e1b50869d-3"></a>
<a name="rest_code_f6ab3385093946ef8e3f4c3e1b50869d-4"></a><span class="n">z_mu</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_f6ab3385093946ef8e3f4c3e1b50869d-5"></a><span class="n">z_log_var</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_f6ab3385093946ef8e3f4c3e1b50869d-6"></a><span class="n">z_sigma</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="o">*</span><span class="n">t</span><span class="p">))(</span><span class="n">z_log_var</span><span class="p">)</span>
</pre>
<div class="figure align-center">
<object data="../../images/vae/encoder.svg" style="height: 500px;" type="image/svg+xml">
../../images/vae/encoder.svg</object>
<p class="caption">Encoder architecture.</p>
</div>
</div>
<div class="section" id="kl-divergence">
<h4>KL Divergence</h4>
<p>We choose prior <span class="math">\(p(\mathbf{z})\)</span> to be</p>
<div class="math">
\begin{equation*}
p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I}).
\end{equation*}
</div>
<p>latent space regularization</p>
<div class="math">
\begin{equation*}
\mathrm{KL} [q_{\phi}(\mathbf{z} | \mathbf{x}) \| \log p(\mathbf{z}) ]
= - \frac{1}{2} \sum_{k=1}^K \{ 1 + \log \sigma_k^2 - \mu_k^2 - \sigma_k^2 \}
\end{equation*}
</div>
<pre class="code python"><a name="rest_code_a9959984f1734350b5687eb7f025f1be-1"></a><span class="k">class</span> <span class="nc">KLDivergenceLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
<a name="rest_code_a9959984f1734350b5687eb7f025f1be-2"></a>    <span class="sd">"""</span>
<a name="rest_code_a9959984f1734350b5687eb7f025f1be-3"></a><span class="sd">    Identity layer that adds KL divergence to the final model loss.</span>
<a name="rest_code_a9959984f1734350b5687eb7f025f1be-4"></a><span class="sd">    """</span>
<a name="rest_code_a9959984f1734350b5687eb7f025f1be-5"></a>
<a name="rest_code_a9959984f1734350b5687eb7f025f1be-6"></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<a name="rest_code_a9959984f1734350b5687eb7f025f1be-7"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">is_placeholder</span> <span class="o">=</span> <span class="bp">True</span>
<a name="rest_code_a9959984f1734350b5687eb7f025f1be-8"></a>        <span class="nb">super</span><span class="p">(</span><span class="n">KLDivergenceLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<a name="rest_code_a9959984f1734350b5687eb7f025f1be-9"></a>
<a name="rest_code_a9959984f1734350b5687eb7f025f1be-10"></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
<a name="rest_code_a9959984f1734350b5687eb7f025f1be-11"></a>
<a name="rest_code_a9959984f1734350b5687eb7f025f1be-12"></a>        <span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span> <span class="o">=</span> <span class="n">inputs</span>
<a name="rest_code_a9959984f1734350b5687eb7f025f1be-13"></a>
<a name="rest_code_a9959984f1734350b5687eb7f025f1be-14"></a>        <span class="n">kl</span> <span class="o">=</span> <span class="o">-</span> <span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">log_var</span> <span class="o">-</span>
<a name="rest_code_a9959984f1734350b5687eb7f025f1be-15"></a>                          <span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span> <span class="o">-</span>
<a name="rest_code_a9959984f1734350b5687eb7f025f1be-16"></a>                          <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_var</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a name="rest_code_a9959984f1734350b5687eb7f025f1be-17"></a>
<a name="rest_code_a9959984f1734350b5687eb7f025f1be-18"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">kl</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span>
<a name="rest_code_a9959984f1734350b5687eb7f025f1be-19"></a>
<a name="rest_code_a9959984f1734350b5687eb7f025f1be-20"></a>        <span class="k">return</span> <span class="n">inputs</span>
</pre>
<pre class="code python"><a name="rest_code_f6970aa15522422aa24e4ebe88317c11-1"></a><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">KLDivergenceLayer</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">])</span>
</pre>
<p>by itself, it will learn to ignore the input and map all outputs to 0.
It is only when we tack on the decoder that the reconstruction likelihood
is introduced. Only then will we reconcile the likelihood / observed data with
our prior to form the posterior over latent codes.</p>
<p>At this stage we could specify
<tt class="docutils literal">prob_encoder = Model(inputs=x, <span class="pre">outputs=[z_mu,</span> z_sigma])</tt>
and compile it with something like
<tt class="docutils literal"><span class="pre">prob_encoder.compile(optimizer='rmsprop`,</span> loss=None)</tt>.
When we fit it, it would trivially map all inputs to 0 and 1, thus learning the
prior distribution.</p>
<div class="figure align-center">
<object data="../../images/vae/encoder_full.svg" style="height: 500px;" type="image/svg+xml">
../../images/vae/encoder_full.svg</object>
<p class="caption">Full encoder architecture, including auxiliary KL divergence layer.</p>
</div>
</div>
</div>
<div class="section" id="decoder">
<h3>Decoder</h3>
<pre class="code python"><a name="rest_code_58ce2502842243f4b843f1c32788d0fa-1"></a><span class="n">decoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
<a name="rest_code_58ce2502842243f4b843f1c32788d0fa-2"></a>    <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span>
<a name="rest_code_58ce2502842243f4b843f1c32788d0fa-3"></a>          <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
<a name="rest_code_58ce2502842243f4b843f1c32788d0fa-4"></a>    <span class="n">Dense</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">)</span>
<a name="rest_code_58ce2502842243f4b843f1c32788d0fa-5"></a><span class="p">])</span>
</pre>
<div class="figure align-center">
<object data="../../images/vae/decoder.svg" style="height: 200px;" type="image/svg+xml">
../../images/vae/decoder.svg</object>
<p class="caption">Decoder architecture.</p>
</div>
<p>Bayesian softmax regression with amortized variational inference</p>
</div>
<div class="section" id="putting-it-all-together">
<h3>Putting it all together</h3>
<pre class="code python"><a name="rest_code_01e0e0a0583c472ba634ffa384425613-1"></a><span class="n">x</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,))</span>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-2"></a><span class="n">h</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-3"></a>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-4"></a><span class="n">z_mu</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-5"></a><span class="n">z_log_var</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-6"></a>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-7"></a><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">KLDivergenceLayer</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">])</span>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-8"></a><span class="n">z_sigma</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="o">*</span><span class="n">t</span><span class="p">))(</span><span class="n">z_log_var</span><span class="p">)</span>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-9"></a>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-10"></a><span class="n">eps</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,))</span>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-11"></a><span class="n">z_eps</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">()([</span><span class="n">z_sigma</span><span class="p">,</span> <span class="n">eps</span><span class="p">])</span>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-12"></a><span class="n">z</span> <span class="o">=</span> <span class="n">Add</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_eps</span><span class="p">])</span>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-13"></a>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-14"></a><span class="n">decoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-15"></a>    <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-16"></a>          <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-17"></a>    <span class="n">Dense</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">)</span>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-18"></a><span class="p">])</span>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-19"></a>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-20"></a><span class="n">vae</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">eps</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
<a name="rest_code_01e0e0a0583c472ba634ffa384425613-21"></a><span class="n">vae</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'rmsprop'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">nll</span><span class="p">)</span>
</pre>
<div class="figure align-center">
<object data="../../images/vae/vae_full_shapes.svg" style="height: 500px;" type="image/svg+xml">
../../images/vae/vae_full_shapes.svg</object>
<p class="caption">Variational autoencoder architecture.</p>
</div>
<p>The point of this tutorial is to illustrate the general framework for performing
amortized variational inference using Keras, treating the inference network
(approximate posterior) and the generative network (likelihood) as black-boxes.
What we've used for the encoder and decoder each with a single hidden
full-connected layer is perhaps the minimal viable architecture.
In the examples directory, Keras provides a more sophisticated variational
autoencoder with deconvolutional layers. The architecture definitions can be
trivially copy-pasted here without need to modify anything else.</p>
</div>
</div>
<div class="section" id="model-fitting">
<h2>Model fitting</h2>
<p>We load the training data as usual. Now the <tt class="docutils literal">vae</tt> is explicitly specified with
random noise source as an auxiliary input. This allows to easily control the
base distribution <span class="math">\(p(\mathbf{\epsilon})\)</span> and also how we draw Monte Carlo
samples of <span class="math">\(\mathbf{z}\)</span> for each datapoint <span class="math">\(\mathbf{x}\)</span>. Usually
we just stick with a simple isotropic Gaussian distribution and draw a different
MC sample for each datapoint.</p>
<pre class="code python"><a name="rest_code_328f2f9b004f4493875c93730a8ebcdc-1"></a><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<a name="rest_code_328f2f9b004f4493875c93730a8ebcdc-2"></a><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">original_dim</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
<a name="rest_code_328f2f9b004f4493875c93730a8ebcdc-3"></a><span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">original_dim</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
<a name="rest_code_328f2f9b004f4493875c93730a8ebcdc-4"></a>
<a name="rest_code_328f2f9b004f4493875c93730a8ebcdc-5"></a><span class="n">eps_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">),</span> <span class="n">latent_dim</span><span class="p">)</span>
<a name="rest_code_328f2f9b004f4493875c93730a8ebcdc-6"></a><span class="n">eps_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_test</span><span class="p">),</span> <span class="n">latent_dim</span><span class="p">)</span>
</pre>
<p>Model fitting feels less intuitive. The <tt class="docutils literal">vae</tt> is compiled with <tt class="docutils literal">loss=None</tt>
explicitly specified which raises a warning. When fit is called, the targets
argument is left unspecified, and the reconstruction loss is optimized through
the <cite>CustomLayer</cite>. This mapping from mathematical problem formulation to code
implementation appears more natural and straightforward. It's easy to understand
at a glance from our call to the <tt class="docutils literal">fit</tt> method that we're training a
probabilistic auto-encoder.</p>
<pre class="code python"><a name="rest_code_e63f7cff8d17488d85f59562b6aa59d9-1"></a><span class="n">vae</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
<a name="rest_code_e63f7cff8d17488d85f59562b6aa59d9-2"></a>    <span class="p">[</span><span class="n">x_train</span><span class="p">,</span> <span class="n">eps_train</span><span class="p">],</span>
<a name="rest_code_e63f7cff8d17488d85f59562b6aa59d9-3"></a>    <span class="n">x_train</span><span class="p">,</span>
<a name="rest_code_e63f7cff8d17488d85f59562b6aa59d9-4"></a>    <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<a name="rest_code_e63f7cff8d17488d85f59562b6aa59d9-5"></a>    <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
<a name="rest_code_e63f7cff8d17488d85f59562b6aa59d9-6"></a>    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<a name="rest_code_e63f7cff8d17488d85f59562b6aa59d9-7"></a>    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span>
<a name="rest_code_e63f7cff8d17488d85f59562b6aa59d9-8"></a>        <span class="p">[</span><span class="n">x_test</span><span class="p">,</span> <span class="n">eps_test</span><span class="p">],</span>
<a name="rest_code_e63f7cff8d17488d85f59562b6aa59d9-9"></a>        <span class="n">x_test</span>
<a name="rest_code_e63f7cff8d17488d85f59562b6aa59d9-10"></a>    <span class="p">)</span>
<a name="rest_code_e63f7cff8d17488d85f59562b6aa59d9-11"></a><span class="p">)</span>
</pre>
</div>
<div class="section" id="model-evaluation">
<h2>Model evaluation</h2>
</div>
<div class="section" id="what-s-next">
<h2>What's next</h2>
<p>Normalizing flows</p>
<p>We illustrate how to employ the simple Gumbel-Softmax reparameterization to
build a Categorical VAE with discrete latent variables.</p>
<p>We can easily extend <tt class="docutils literal">KLDivergenceLayer</tt> to use an auxiliary density ratio
estimator function, instead of evaluating the KL divergence in closed-form.
This relaxes the requirement on approximate posterior
<span class="math">\(q(\mathbf{z}|\mathbf{x})\)</span> (and incidentally prior <span class="math">\(p(\mathbf{z})\)</span>)
to yield tractable densities, at the cost of maximizing a cruder estimate of the
ELBO.
This is known as Adversarial Variational Bayes <a class="citation-reference" href="#mescheder-et-al-2017" id="id1">[Mescheder_et_al_2017]</a>, and is
an important line of recent research that extends the applicability of
variational inference to arbitrarily expressive implicit probabilistic models
<a class="citation-reference" href="#tran-et-al-2017" id="id2">[Tran_et_al_2017]</a>.</p>
</div>
<div class="section" id="references">
<h2>References</h2>
<table class="docutils citation" frame="void" id="mescheder-et-al-2017" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id1">[Mescheder_et_al_2017]</a></td>
<td>L. Mescheder, S. Nowozin, and A. Geiger,
"Adversarial Variational Bayes: Unifying Variational Autoencoders and
Generative Adversarial Networks," in Proceedings of the 34th International
Conference on Machine Learning, 2017, vol. 70, pp. 2391â€“2400.</td>
</tr></tbody>
</table>
<table class="docutils citation" frame="void" id="tran-et-al-2017" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id2">[Tran_et_al_2017]</a></td>
<td>D. Tran, R. Ranganath, and D. Blei,
"Hierarchical Implicit Models and Likelihood-Free Variational Inference,"
<em>to appear in</em> Advances in Neural Information Processing Systems 30.</td>
</tr></tbody>
</table>
</div>
<div class="section" id="appendix">
<h2>Appendix</h2>
<p>The accompanying Jupyter Notebook used to generate the diagrams and plots can
be found <a class="reference external" href="../../listings/vae/variational_autoencoder.ipynb.html">here</a>.
The fully executable code is reproduced below for completeness.</p>
<p><a class="reference external" href="../../listings/vae/variational_autoencoder_improved.py.html">vae/variational_autoencoder_improved.py</a>  <a class="reference external" href="../../listings/vae/variational_autoencoder_improved.py">(Source)</a></p>
<pre class="code python"><a name="rest_code_053c38146e9841a5b724ab11bd591f49-1"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-2"></a><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-3"></a><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-4"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-5"></a><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">,</span> <span class="n">Layer</span><span class="p">,</span> <span class="n">Add</span><span class="p">,</span> <span class="n">Multiply</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-6"></a><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">Sequential</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-7"></a><span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-8"></a><span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-9"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-10"></a><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-11"></a><span class="n">original_dim</span> <span class="o">=</span> <span class="mi">784</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-12"></a><span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">2</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-13"></a><span class="n">intermediate_dim</span> <span class="o">=</span> <span class="mi">256</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-14"></a><span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-15"></a><span class="n">epsilon_std</span> <span class="o">=</span> <span class="mf">1.0</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-16"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-17"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-18"></a><span class="k">def</span> <span class="nf">nll</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-19"></a>    <span class="sd">""" Negative log likelihood. """</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-20"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-21"></a>    <span class="c1"># keras.losses.binary_crossentropy gives the mean</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-22"></a>    <span class="c1"># over the last axis. We require the sum.</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-23"></a>    <span class="k">return</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">binary_crossentropy</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-24"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-25"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-26"></a><span class="k">class</span> <span class="nc">KLDivergenceLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-27"></a>    <span class="sd">"""</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-28"></a><span class="sd">    Identity layer that adds KL divergence to the final model loss.</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-29"></a><span class="sd">    """</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-30"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-31"></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-32"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">is_placeholder</span> <span class="o">=</span> <span class="bp">True</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-33"></a>        <span class="nb">super</span><span class="p">(</span><span class="n">KLDivergenceLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-34"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-35"></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-36"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-37"></a>        <span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span> <span class="o">=</span> <span class="n">inputs</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-38"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-39"></a>        <span class="n">kl</span> <span class="o">=</span> <span class="o">-</span> <span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">log_var</span> <span class="o">-</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-40"></a>                          <span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span> <span class="o">-</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-41"></a>                          <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_var</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-42"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-43"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">kl</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-44"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-45"></a>        <span class="k">return</span> <span class="n">inputs</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-46"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-47"></a><span class="n">x</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,))</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-48"></a><span class="n">h</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-49"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-50"></a><span class="n">z_mu</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-51"></a><span class="n">z_log_var</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-52"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-53"></a><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">KLDivergenceLayer</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">])</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-54"></a><span class="n">z_sigma</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="o">*</span><span class="n">t</span><span class="p">))(</span><span class="n">z_log_var</span><span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-55"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-56"></a><span class="n">eps</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,))</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-57"></a><span class="n">z_eps</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">()([</span><span class="n">z_sigma</span><span class="p">,</span> <span class="n">eps</span><span class="p">])</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-58"></a><span class="n">z</span> <span class="o">=</span> <span class="n">Add</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_eps</span><span class="p">])</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-59"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-60"></a><span class="n">encoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z_mu</span><span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-61"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-62"></a><span class="n">decoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-63"></a>    <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-64"></a>          <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-65"></a>    <span class="n">Dense</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-66"></a><span class="p">])</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-67"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-68"></a><span class="n">vae</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">eps</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-69"></a><span class="n">vae</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'rmsprop'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">nll</span><span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-70"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-71"></a><span class="c1"># train the VAE on MNIST digits</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-72"></a><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-73"></a><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">original_dim</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-74"></a><span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">original_dim</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-75"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-76"></a><span class="n">eps_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">),</span> <span class="n">latent_dim</span><span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-77"></a><span class="n">eps_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_test</span><span class="p">),</span> <span class="n">latent_dim</span><span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-78"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-79"></a><span class="n">vae</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-80"></a>    <span class="p">[</span><span class="n">x_train</span><span class="p">,</span> <span class="n">eps_train</span><span class="p">],</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-81"></a>    <span class="n">x_train</span><span class="p">,</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-82"></a>    <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-83"></a>    <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-84"></a>    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-85"></a>    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-86"></a>        <span class="p">[</span><span class="n">x_test</span><span class="p">,</span> <span class="n">eps_test</span><span class="p">],</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-87"></a>        <span class="n">x_test</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-88"></a>    <span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-89"></a><span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-90"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-91"></a><span class="c1"># display a 2D plot of the digit classes in the latent space</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-92"></a><span class="n">x_test_encoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-93"></a><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-94"></a><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_test_encoded</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_test_encoded</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-95"></a>            <span class="n">c</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">4</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">3</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'viridis'</span><span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-96"></a><span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-97"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-98"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-99"></a><span class="c1"># display a 2D manifold of the digits</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-100"></a><span class="n">n</span> <span class="o">=</span> <span class="mi">15</span>  <span class="c1"># figure with 15x15 digits</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-101"></a><span class="n">digit_size</span> <span class="o">=</span> <span class="mi">28</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-102"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-103"></a><span class="c1"># linearly spaced coordinates on the unit square were transformed</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-104"></a><span class="c1"># through the inverse CDF (ppf) of the Gaussian to produce values</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-105"></a><span class="c1"># of the latent variables z, since the prior of the latent space</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-106"></a><span class="c1"># is Gaussian</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-107"></a><span class="n">u_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-108"></a>                               <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">n</span><span class="p">)))</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-109"></a><span class="n">z_grid</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">u_grid</span><span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-110"></a><span class="n">x_decoded</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">z_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-111"></a><span class="n">x_decoded</span> <span class="o">=</span> <span class="n">x_decoded</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">digit_size</span><span class="p">,</span> <span class="n">digit_size</span><span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-112"></a>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-113"></a><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-114"></a><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">x_decoded</span><span class="p">))),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
<a name="rest_code_053c38146e9841a5b724ab11bd591f49-115"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>
<div class="section" id="number-of-monte-carlo-samples">
<h3>Number of Monte Carlo samples</h3>
<pre class="code python"><a name="rest_code_d25804514a4047519309e3c732bb1dff-1"></a><span class="n">eps</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">mc_samples</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">))</span>
</pre>
<p>Everything else remains exactly the same. The <tt class="docutils literal">Multiply</tt> layer will
automatically broadcast <tt class="docutils literal">eps</tt> which has shape
<tt class="docutils literal">(batch_size, mc_samples, latent_dim)</tt> with <tt class="docutils literal">sigma</tt> which has shape
<tt class="docutils literal">(batch_size, latent_dim)</tt> and output shape
<tt class="docutils literal">(batch_size, mc_samples, latent_dim)</tt>. Since the subsequent layers do not
operate on the which will then be propagated to the
final output.</p>
<p>diagram here</p>
<p>We expand the targets to 3d a array <tt class="docutils literal">np.expand_dims(x_train, axis=1)</tt> to be
of shape <tt class="docutils literal">(batch_size, 1, original_dim)</tt> so that the loss function can
broadcast with the output with shape <tt class="docutils literal">(batch_size, mc_samples, original_dim)</tt>.
It is important to make the distinction between the log likelihood of the mean
over outputs, versus the mean of the log likelihood over the outputs. Since we
require the expected log likelihood, we are interested in the latter.</p>
<pre class="code python"><a name="rest_code_85986f517eac4100932b905996a67d45-1"></a><span class="n">eps_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">),</span> <span class="n">mc_samples</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
<a name="rest_code_85986f517eac4100932b905996a67d45-2"></a><span class="n">eps_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_test</span><span class="p">),</span> <span class="n">mc_samples</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
<a name="rest_code_85986f517eac4100932b905996a67d45-3"></a>
<a name="rest_code_85986f517eac4100932b905996a67d45-4"></a><span class="n">vae</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
<a name="rest_code_85986f517eac4100932b905996a67d45-5"></a>    <span class="p">[</span><span class="n">x_train</span><span class="p">,</span> <span class="n">eps_train</span><span class="p">],</span>
<a name="rest_code_85986f517eac4100932b905996a67d45-6"></a>    <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
<a name="rest_code_85986f517eac4100932b905996a67d45-7"></a>    <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<a name="rest_code_85986f517eac4100932b905996a67d45-8"></a>    <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
<a name="rest_code_85986f517eac4100932b905996a67d45-9"></a>    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<a name="rest_code_85986f517eac4100932b905996a67d45-10"></a>    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span>
<a name="rest_code_85986f517eac4100932b905996a67d45-11"></a>        <span class="p">[</span><span class="n">x_test</span><span class="p">,</span> <span class="n">eps_test</span><span class="p">],</span>
<a name="rest_code_85986f517eac4100932b905996a67d45-12"></a>        <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a name="rest_code_85986f517eac4100932b905996a67d45-13"></a>    <span class="p">)</span>
<a name="rest_code_85986f517eac4100932b905996a67d45-14"></a><span class="p">)</span>
</pre>
<p>For every data point, there are <tt class="docutils literal">mc_samples</tt> reconstructions.</p>
<pre class="code python"><a name="rest_code_44a619ba378c46ab8e681246cbdce0da-1"></a><span class="n">recons</span> <span class="o">=</span> <span class="n">vae</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">x_test</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">eps_test</span><span class="p">[:</span><span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
<a name="rest_code_44a619ba378c46ab8e681246cbdce0da-2"></a>
<a name="rest_code_44a619ba378c46ab8e681246cbdce0da-3"></a><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<a name="rest_code_44a619ba378c46ab8e681246cbdce0da-4"></a><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">recons</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)))),</span>
<a name="rest_code_44a619ba378c46ab8e681246cbdce0da-5"></a>           <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
<a name="rest_code_44a619ba378c46ab8e681246cbdce0da-6"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>
<p>plot here</p>
</div>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../tags/deep-learning/" rel="tag">deep learning</a></li>
            <li><a class="tag p-category" href="../../tags/keras/" rel="tag">keras</a></li>
            <li><a class="tag p-category" href="../../tags/python/" rel="tag">python</a></li>
            <li><a class="tag p-category" href="../../tags/representation-learning/" rel="tag">representation learning</a></li>
            <li><a class="tag p-category" href="../../tags/tensorflow/" rel="tag">tensorflow</a></li>
            <li><a class="tag p-category" href="../../tags/unsupervised-learning/" rel="tag">unsupervised learning</a></li>
            <li><a class="tag p-category" href="../../tags/variational-autoencoder/" rel="tag">variational autoencoder</a></li>
            <li><a class="tag p-category" href="../../tags/variational-inference/" rel="tag">variational inference</a></li>
        </ul></nav></aside><section class="comments hidden-print"><h2>Comments</h2>
                        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="ltiao",
            disqus_url="http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/",
        disqus_title="Implementing Variational Autoencoders in Keras: Beyond the Quickstart Tutorial",
        disqus_identifier="cache/content/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha256-SDRP1VVYu+tgAGKhddBSl5+ezofHKZeI+OzxakbIe/Y=" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></article><script>var disqus_shortname="ltiao";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>
        <!--End of body content-->
      </div>
    </div>

    <footer id="footer" class="footer">
        
Contents Â© 2017
<a href="mailto:louistiao@me.com">Louis Tiao</a> - Powered by
<a href="https://getnikola.com" rel="nofollow">Nikola</a>


<span class="pull-right">

  <a class="twitter-follow-button" href="https://twitter.com/louistiao" data-show-count="false" data-show-screen-name="false">
  Follow @louistiao
  </a>

  <a class="github-button" href="https://github.com/ltiao" aria-label="Follow @ltiao on GitHub" data-show-count="false">
  Follow @ltiao
  </a>

  <a href="https://ko-fi.com/A3476EX">
    <object type="image/svg+xml" style="pointer-events: none;" data="https://img.shields.io/badge/Support--yellow.svg?style=social"></object>
  </a>

</span>


            
    </footer>
</div> <!-- /container -->

            <script src="../../assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><!-- Google Analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-43722566-1', 'auto');
  ga('send', 'pageview');

</script><!-- GitHub Buttons --><script async defer src="https://buttons.github.io/buttons.js"></script><!-- Twitter Widgets --><script>window.twttr = (function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0],
    t = window.twttr || {};
  if (d.getElementById(id)) return t;
  js = d.createElement(s);
  js.id = id;
  js.src = "https://platform.twitter.com/widgets.js";
  fjs.parentNode.insertBefore(js, fjs);

  t._e = [];
  t.ready = function(f) {
    t._e.push(f);
  };

  return t;
}(document, "script", "twitter-wjs"));</script>
</body>
</html>
