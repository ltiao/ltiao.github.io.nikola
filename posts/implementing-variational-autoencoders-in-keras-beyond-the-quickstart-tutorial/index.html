<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns#
article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Implementing Variational Autoencoders in Keras: Beyond the Quickstart Tutorial | Louis Tiao</title>
<link href="../../assets/css/all.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/override_nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/">
<link rel="icon" href="../../favicon_16x16.ico" sizes="16x16">
<link rel="icon" href="../../favicon_32x32.ico" sizes="32x32">
<link rel="icon" href="../../favicon_256x256.ico" sizes="256x256">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><!--

    /\\\\\\\\\\\\\\\  /\\\\\\\\\\\     /\\\\\\\\\          /\\\\\              
    \///////\\\/////  \/////\\\///    /\\\\\\\\\\\\\      /\\\///\\\           
           \/\\\           \/\\\      /\\\/////////\\\   /\\\/  \///\\\        
            \/\\\           \/\\\     \/\\\       \/\\\  /\\\      \//\\\      
             \/\\\           \/\\\     \/\\\\\\\\\\\\\\\ \/\\\       \/\\\     
              \/\\\           \/\\\     \/\\\/////////\\\ \//\\\      /\\\     
               \/\\\           \/\\\     \/\\\       \/\\\  \///\\\  /\\\      
                \/\\\        /\\\\\\\\\\\ \/\\\       \/\\\    \///\\\\\/      
                 \///        \///////////  \///        \///       \/////       

--><meta name="author" content="Louis Tiao">
<link rel="prev" href="../../notes/working-with-samples-of-distributions-over-convolutional-kernels/" title="Working with Samples of Distributions over Convolutional Kernels" type="text/html">
<link rel="next" href="../../notes/working-with-pandas-multiindex-dataframes-reading-and-writing-to-csv-and-hdf5/" title="Working with Pandas MultiIndex Dataframes: Reading and Writing to CSV and HDF5" type="text/html">
<meta property="og:site_name" content="Louis Tiao">
<meta property="og:title" content="Implementing Variational Autoencoders in Keras: Beyond the Quickstart ">
<meta property="og:url" content="http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/">
<meta property="og:description" content="Draft
Please do not share or link.

Keras is awesome. It is a very well-designed library that clearly abides by
its guiding principles of modularity and extensibility, and allows us to
easily assemble">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-10-23T01:19:59+11:00">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="keras">
<meta property="article:tag" content="mathjax">
<meta property="article:tag" content="python">
<meta property="article:tag" content="representation learning">
<meta property="article:tag" content="tensorflow">
<meta property="article:tag" content="unsupervised learning">
<meta property="article:tag" content="variational autoencoder">
<meta property="article:tag" content="variational inference">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

  <div class="container">
    <div class="header clearfix">
      <nav><ul class="nav nav-pills pull-right">
<li>
<a href="../../">About</a>
                </li>
<li>
<a href="../../projects/">Projects</a>
                </li>
<li>
<a href="../">Posts</a>
                </li>
<li>
<a href="../../archive.html">Archive</a>

          
        </li>
</ul></nav><a href="http://louistiao.me/">

        <h3 class="text-muted">
          <span id="blog-title">Louis Tiao</span>
        </h3>
      </a>
    </div>

<!-- TODO Figure out what to do with this stuff -->
<!--     <div class="row">

      <ul class="nav nav-pills pull-right">
    <li>
    <a href="/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/index.rst" id="sourcelink">Source</a>
    </li>
          
      </ul>
    </div> -->

    <div id="content" role="main">
      <div class="body-content">
        <!--Body content-->
        <div class="row">
          
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Implementing Variational Autoencoders in Keras: Beyond the Quickstart Tutorial</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    Louis Tiao
            </span></p>
            <p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2017-10-23T01:19:59+11:00" itemprop="datePublished" title="2017-10-23 01:19">2017-10-23 01:19</time></a></p>
                <p class="commentline">            <a href="#disqus_thread" data-disqus-identifier="cache/content/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial.html">Comments</a>


                    </p>
<p class="sourceline"><a href="index.rst" class="sourcelink">Source</a></p>

        </div>
        
    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<div class="admonition admonition-draft">
<p class="first admonition-title">Draft</p>
<p class="last">Please do not share or link.</p>
</div>
<p><a class="reference external" href="https://keras.io/">Keras</a> is awesome. It is a very well-designed library that clearly abides by
its <a class="reference external" href="https://keras.io/#guiding-principles">guiding principles</a> of modularity and extensibility, and allows us to
easily assemble powerful, complex models from primitive building blocks.
This has been demonstrated by many blog posts and tutorials, such as the
excellent tutorial on <a class="reference external" href="https://blog.keras.io/building-autoencoders-in-keras.html">Building Autoencoders in Keras</a>.
As the name suggests, that tutorial provides examples of how to implement
various kinds of autoencoders in Keras, including the variational autoencoder
(VAE) <a class="footnote-reference" href="#kingma2014" id="id1">[1]</a>.</p>
<div class="figure align-center">
<img alt="../../images/vae/result_combined.png" src="../../images/vae/result_combined.png"><p class="caption">Visualization of 2D manifold of MNIST digits (left)
and the representation of digits in latent space colored according to their
digit labels (right).</p>
</div>
<p>Like all autoencoders, the variational autoencoder is primarily used for
unsupervised learning of hidden representations.
However, they are fundamentally different to your usual neural network-based
autoencoder in that they approach the problem from a probabilistic perspective:
by specifying a joint distribution over the observed and latent variables, and
approximating the intractable posterior conditional density over latent
variables with variational inference, using an <em>inference network</em>
<a class="footnote-reference" href="#inference1" id="id2">[2]</a> <a class="footnote-reference" href="#inference2" id="id3">[3]</a> or more classically, a <em>recognition model</em>
<a class="footnote-reference" href="#dayan1995" id="id4">[4]</a> to amortize the cost of inference.</p>
<!-- TEASER_END -->
<p>While the examples in the aforementioned tutorial do well to showcase the
versatility of Keras on a wide range of autoencoder model architectures,
<a class="reference external" href="https://github.com/fchollet/keras/blob/2.0.8/examples/variational_autoencoder.py">its implementation of the variational autoencoder</a> doesn't properly take
advantage of Keras' modular design, making it difficult to generalize and
extend in important ways. As we will see, it relies on implementing custom
layers and constructs that are restricted to a specific instance of
variational autoencoders. This is a shame because when combined, Keras'
building blocks are powerful enough to encapsulate most variants of the
variational autoencoder and more generally, recognition-generative model
combinations for which the generative model belongs to a large family of
<em>deep latent Gaussian models</em> (DLGMs) <a class="footnote-reference" href="#rezende2014" id="id5">[5]</a>.</p>
<p>The goal of this post is to propose a clean and elegant alternative
implementation that takes better advantage of Keras' modular design.
It is not a tutorial on variational autoencoders <a class="footnote-reference" href="#id16" id="id6">[*]</a>.
Rather, we study variational autoencoders as a specific case of variational
inference in deep latent Gaussian models with inference networks, and
demonstrate how we can use Keras to implement them in a modular fashion such
that they can be easily adapted to approximate inference in various common
problems with different (non-Gaussian) likelihoods, such as classification with
Bayesian logistic / softmax regression.</p>
<p>This first post will lay the groundwork for a series of future posts that
explore ways to extend this basic modular framework to implement the more
powerful methods proposed in the latest research, such as the normalizing flows
for building richer posterior approximations <a class="footnote-reference" href="#rezende2015" id="id7">[6]</a>, importance weighted
autoencoders <a class="footnote-reference" href="#burda2015" id="id8">[7]</a>, the Gumbel-softmax trick for inference in discrete
latent variables <a class="footnote-reference" href="#jang2016" id="id9">[8]</a>, and even the most recent GAN-based density-ratio
estimation techniques for likelihood-free inference <a class="footnote-reference" href="#mescheder2017" id="id10">[9]</a> <a class="footnote-reference" href="#tran2017" id="id11">[10]</a>.</p>
<div class="section" id="model-specification">
<h2>Model specification</h2>
<p>First, it is important to understand that the variational autoencoder
<a class="reference external" href="http://dustintran.com/blog/variational-auto-encoders-do-not-train-complex-generative-models">is not a way to train generative models</a>.
Rather, the generative model is a component of the variational autoencoder and
is, in general, a deep latent Gaussian model.
In particular, let <span class="math">\(\mathbf{x}\)</span> be a local observed variable and
<span class="math">\(\mathbf{z}\)</span> its corresponding local latent variable, with joint
distribution</p>
<div class="math">
\begin{equation*}
p_{\theta}(\mathbf{x}, \mathbf{z})
= p_{\theta}(\mathbf{x} | \mathbf{z}) p(\mathbf{z}).
\end{equation*}
</div>
<p>In Bayesian modelling, we assume the distribution of observed variables to be
governed by the latent variables. Latent variables are drawn from a prior
density <span class="math">\(p(\mathbf{z})\)</span> and related to the observations though the
likelihood <span class="math">\(p_{\theta}(\mathbf{x} | \mathbf{z})\)</span>.
Deep latent Gaussian models (DLGMs) are a general class of models where the
observed variable is governed by a <em>hierarchy</em> of latent variables, and the
latent variables at each level of the hierarchy are Gaussian <em>a priori</em>
<a class="footnote-reference" href="#rezende2014" id="id12">[5]</a>.</p>
<p>In a typical instance of the variational autoencoder, we have only a single
level of latent variables whose prior distributions are Gaussian,</p>
<div class="math">
\begin{equation*}
p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I}).
\end{equation*}
</div>
<p>Now, each local latent variable is related to its corresponding observation
through the likelihood <span class="math">\(p_{\theta}(\mathbf{x} | \mathbf{z})\)</span>, which can
be viewed as a <em>probabilistic</em> decoder: conditioned on a hidden
lower-dimensional representation, or code, <span class="math">\(\mathbf{z}\)</span>, it decodes it
into a <em>probability distribution</em> over the observation <span class="math">\(\mathbf{x}\)</span>.</p>
<div class="section" id="decoder">
<h3>Decoder</h3>
<p>In this example, we define <span class="math">\(p_{\theta}(\mathbf{x} | \mathbf{z})\)</span> to
be a multivariate Bernoulli whose probabilities are computed from
<span class="math">\(\mathbf{z}\)</span> using a fully-connected neural network with a single hidden
layer,</p>
<div class="math">
\begin{align*}
p_{\theta}(\mathbf{x} | \mathbf{z})
  &amp; = \mathrm{Bern}( \sigma( \mathbf{W}_2 \mathbf{h} + \mathbf{b}_2 ) ), \\
\mathbf{h} &amp; = h(\mathbf{W}_1 \mathbf{z} + \mathbf{b}_1),
\end{align*}
</div>
<p>where <span class="math">\(\sigma\)</span> is the logistic sigmoid function, <span class="math">\(h\)</span> is some
non-linearity, and the model parameters
<span class="math">\(\theta = \{ \mathbf{W}_1, \mathbf{W}_2, \mathbf{b}_1, \mathbf{b}_1 \}\)</span>
consist of the weights and biases of this neural network.</p>
<p>It is straightforward to implement this in Keras with the
<a class="reference external" href="https://keras.io/models/sequential/">Sequential model API</a>:</p>
<pre class="code python"><a name="rest_code_01efb54b54854fe5a45dbdb4cf33987d-1"></a><span class="n">decoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
<a name="rest_code_01efb54b54854fe5a45dbdb4cf33987d-2"></a>  <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
<a name="rest_code_01efb54b54854fe5a45dbdb4cf33987d-3"></a>  <span class="n">Dense</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">)</span>
<a name="rest_code_01efb54b54854fe5a45dbdb4cf33987d-4"></a><span class="p">])</span>
</pre>
<p>You can view a summary of the model parameters <span class="math">\(\theta\)</span> by calling
<tt class="docutils literal">decoder.summary()</tt>. Additionally, you can produce a high-level diagram of
the network architecture, and optionally the input and output shapes of each
layer using <a class="reference external" href="https://keras.io/visualization/">plot_model</a> from the
<tt class="docutils literal">keras.utils.vis_utils</tt> module. Although our architecture is about as
simple as it gets, it is included in the figure below as an example of what
the diagrams look like.</p>
<div class="figure align-center">
<object data="../../images/vae/decoder.svg" style="height: 200px;" type="image/svg+xml">
../../images/vae/decoder.svg</object>
<p class="caption">Decoder architecture.</p>
</div>
<p>Note that by fixing <span class="math">\(\mathbf{W}_1\)</span>, <span class="math">\(\mathbf{b}_1\)</span> and <span class="math">\(h\)</span>
to be the identity matrix, the zero vector, and the identity function,
respectively (or equivalently dropping the first <tt class="docutils literal">Dense</tt> layer in the snippet
above altogether), we recover <em>logistic factor analysis</em>.
With similarly minor modifications, we can recover other members from the
family of DLGMs, which include <em>non-linear factor analysis</em>,
<em>non-linear Gaussian belief networks</em>, <em>sigmoid belief networks</em>, and many
others <a class="footnote-reference" href="#rezende2014" id="id13">[5]</a>.</p>
<p>Having specified the generative process, we would now like to perform inference
on the latent variables and model parameters <span class="math">\(\mathbf{z}\)</span> and <span class="math">\(\theta\)</span>.
In particular, our goal is to compute the posterior
<span class="math">\(p_{\theta}(\mathbf{z} | \mathbf{x})\)</span>, the conditional density of
latent variable <span class="math">\(\mathbf{z}\)</span> given the observed variable
<span class="math">\(\mathbf{x}\)</span>.</p>
<p>Which requires us marginalize out the latent variables <span class="math">\(\mathbf{z}\)</span> to
obtain the marginal likelihood <span class="math">\(p_{\theta}(\mathbf{x})\)</span> and to maximize
it with respect to the model parameters <span class="math">\(\theta\)</span>.</p>
<p>When combined end-to-end, the inference network and the deep latent Gaussian
model can be seen as having an autoencoder structure.
Indeed, this general structure contains the variational autoencoder as a special
case, and more traditionally, the Helmholtz machine.
Even more generally, we can use this structure to perform amortized variational
inference in complex generative models for a wide array of supervised,
unsupervised and semi-supervised tasks.</p>
<p>The loss we wish to minimize is the <em>negative</em> of the <em>evidence lower bound</em>
(ELBO), which is expressed as</p>
<div class="math">
\begin{align*}
\mathrm{ELBO}(q)
&amp;=
\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})} [
  \log p_{\theta}(\mathbf{x} | \mathbf{z}) +
  \log p(\mathbf{z}) -
  \log q_{\phi}(\mathbf{z} | \mathbf{x})
] \\
&amp;=
\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})} [
  \log p_{\theta}(\mathbf{x} | \mathbf{z})
] - \mathrm{KL} [q_{\phi}(\mathbf{z} | \mathbf{x}) \| p(\mathbf{z}) ].
\end{align*}
</div>
<p>importantly, the ELBO is a lower bound to the log marginal likelihood, so maximizing
the ELBO approximately maximizes the log marginal likelihood. Additionally,
maximizing it is equivalent to minimizing
<span class="math">\(\mathrm{KL} [q_{\phi}(\mathbf{z} | \mathbf{x}) \| p(\mathbf{z} | \mathbf{x}) ]\)</span>,
which also determines the tightness of the bound.</p>
</div>
<div class="section" id="encoder-the-recognition-model">
<h3>Encoder (the recognition model)</h3>
<p>Every local latent variable x_i corresponding to observed variable x_i has its
own set of local variational parameters phi_i. For example,
q_{phi_i}(z_i) = N(z_i | mu_i, diag(sigma_i^2)), with variational parameters
phi_i = {mu_i, sigma_i}.</p>
<!-- Note that it is not dependent on the observed data x_i -->
<!-- and does not appear in the expression q_i(z_i). It is only related to x_i -->
<!-- through the ELBO. -->
<p>The number of local variational parameters grows with the size
of the observed data. Furthermore, a new set of parameters must be optimized
for unseen test data points. We <em>amortize</em> the cost of inference by introducing
an <em>inference network</em> which outputs the local variational parameters phi_i
given x_i as input. This approximation allows statistical strength to be shared
across observed data-points and also generalize to unseen test points.</p>
<!-- This amortizes inference by only defining a set of global parameters, namely, -->
<!-- the parameters of the neural network. -->
<p>Continuing with the example, we have
q_{phi}(z_i | x_i ) = N(z_i | mu_{phi}(x_i), diag(sigma_{phi}(x_i)^2)),
with variational parameters phi_i = {mu_i, sigma_i}.</p>
<p>In the specific case of autoencoders, the network that maps latent code</p>
<p>More the general case of amortized variational inference, this is known as a
recognition model, or an inference network.</p>
<div class="math">
\begin{equation*}
q_{\phi}(\mathbf{z} | \mathbf{x})
=
\mathcal{N}(
  \mathbf{z} |
  \mathbf{\mu}_{\phi}(\mathbf{x}),
  \mathrm{diag}(\mathbf{\sigma}_{\phi}^2(\mathbf{x}))
)
\end{equation*}
</div>
<pre class="code python"><a name="rest_code_5141057482ca47d79339888229435275-1"></a><span class="n">x</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,))</span>
<a name="rest_code_5141057482ca47d79339888229435275-2"></a><span class="n">h</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<a name="rest_code_5141057482ca47d79339888229435275-3"></a>
<a name="rest_code_5141057482ca47d79339888229435275-4"></a><span class="n">z_mu</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_5141057482ca47d79339888229435275-5"></a><span class="n">z_log_var</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_5141057482ca47d79339888229435275-6"></a><span class="n">z_sigma</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="o">*</span><span class="n">t</span><span class="p">))(</span><span class="n">z_log_var</span><span class="p">)</span>
</pre>
<p><strong>figure here</strong></p>
<div class="section" id="reparameterization-using-keras-layers">
<h4>Reparameterization using Keras Layers</h4>
<p>To perform gradient-based optimization of ELBO, we require its gradients with
respect to the variational parameters <span class="math">\(\phi\)</span>, which is generally
intractable. Currently, the dominant approach for circumventing this is by
Monte Carlo (MC) estimation of the gradients. There are a several estimators
based on different variance reduction techniques. However, for continuous
latent variables, the <em>reparameterization gradients</em> can be shown to have the
lowest variance among competing estimators.</p>
<p>The ELBO can be written as an expectation of a multivariate function
<span class="math">\(f(\mathbf{x}, \mathbf{z}) = \log p_{\theta}(\mathbf{x} , \mathbf{z}) - \log q_{\phi}(\mathbf{z} | \mathbf{x})\)</span>
over distribution <span class="math">\(q_{\phi}(\mathbf{z} | \mathbf{x})\)</span>.</p>
<div class="math">
\begin{align*}
\nabla_{\phi}
\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})} [ f(\mathbf{x}, \mathbf{z}) ]
&amp;= \nabla_{\phi} \mathbb{E}_{p(\mathbf{\epsilon})} [
   f(\mathbf{x},
     g_{\phi}(\mathbf{x}, \mathbf{\epsilon}))
] \\
&amp;= \mathbb{E}_{p(\mathbf{\epsilon})} [
 \nabla_{\phi}
 f(\mathbf{x},
   g_{\phi}(\mathbf{x}, \mathbf{\epsilon}))
] \\
\end{align*}
</div>
<p>Specifying  gives us the gradient of the ELBO above.</p>
<div class="math">
\begin{equation*}
z = g_{\phi}(\mathbf{x}, \mathbf{\epsilon}), \quad
  \mathbf{\epsilon} \sim p(\mathbf{\epsilon})
\end{equation*}
</div>
<div class="math">
\begin{equation*}
g_{\phi}(\mathbf{x}, \mathbf{\epsilon}) =
  \mathbf{\mu}_{\phi}(\mathbf{x}) +
  \mathbf{\sigma}_{\phi}(\mathbf{x}) \odot
  \mathbf{\epsilon}, \quad
  \mathbf{\epsilon} \sim
  \mathcal{N}(\mathbf{0}, \mathbf{I})
\end{equation*}
</div>
<p>Assume <tt class="docutils literal">z_mu</tt> and <tt class="docutils literal">z_sigma</tt> are the outputs of some layers. Then, using
<a class="reference external" href="https://keras.io/layers/merge/">Merge Layers</a>, <tt class="docutils literal">Add</tt> and <tt class="docutils literal">Multiply</tt>:</p>
<pre class="code python"><a name="rest_code_c5c7579070484710a25184bfd67b4c74-1"></a><span class="n">eps</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,))</span>
<a name="rest_code_c5c7579070484710a25184bfd67b4c74-2"></a><span class="n">z_eps</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">()([</span><span class="n">z_sigma</span><span class="p">,</span> <span class="n">eps</span><span class="p">])</span>
<a name="rest_code_c5c7579070484710a25184bfd67b4c74-3"></a>
<a name="rest_code_c5c7579070484710a25184bfd67b4c74-4"></a><span class="n">z</span> <span class="o">=</span> <span class="n">Add</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_eps</span><span class="p">])</span>
</pre>
<div class="figure align-center">
<object data="../../images/vae/reparameterization.svg" style="height: 300px;" type="image/svg+xml">
../../images/vae/reparameterization.svg</object>
<p class="caption">Reparameterization with simple location-scale transformation using Keras
merge layers.</p>
</div>
<p>Lambda layer, which simultaneously draws samples from a hard-coded base
distribution and performs reparameterization. This implementation achieves a
more appropriate level of modularity and abstraction. It's makes it clear that
each of these atomic building blocks are themselves deterministic
transformations which together make up a deterministic transformation.
The source of stochasticity comes from the input, which we are able to tweak at
test time. Gumbel-softmax trick.</p>
<pre class="code python"><a name="rest_code_3c0835467c84436aa80d217d1fc185bd-1"></a><span class="n">eps</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">K</span><span class="o">.</span><span class="n">random_normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">latent_dim</span><span class="p">)))</span>
</pre>
<p>For the sake of illustration, we've fixed <tt class="docutils literal">sigma</tt> and <tt class="docutils literal">mu</tt> as <tt class="docutils literal">Input</tt>
layers. That's why it says <tt class="docutils literal">InputLayer</tt> next to it. In reality, it will be
the output layer of a network. We specify <span class="math">\(\mathbf{\mu}_{\phi}(\mathbf{x})\)</span>
and <span class="math">\(\mathbf{\sigma}_{\phi}(\mathbf{x})\)</span> now.</p>
<div class="figure align-center">
<object data="../../images/vae/encoder.svg" style="height: 500px;" type="image/svg+xml">
../../images/vae/encoder.svg</object>
<p class="caption">Encoder architecture.</p>
</div>
</div>
<div class="section" id="kl-divergence">
<h4>KL Divergence</h4>
<p>latent space regularization</p>
<div class="math">
\begin{equation*}
\mathrm{KL} [q_{\phi}(\mathbf{z} | \mathbf{x}) \| p(\mathbf{z}) ]
= - \frac{1}{2} \sum_{k=1}^K \{ 1 + \log \sigma_k^2 - \mu_k^2 - \sigma_k^2 \}
\end{equation*}
</div>
<pre class="code python"><a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-1"></a><span class="k">class</span> <span class="nc">KLDivergenceLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-2"></a>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-3"></a>    <span class="sd">""" Identity transform layer that adds KL divergence</span>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-4"></a><span class="sd">    to the final model loss.</span>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-5"></a><span class="sd">    """</span>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-6"></a>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-7"></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-8"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">is_placeholder</span> <span class="o">=</span> <span class="bp">True</span>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-9"></a>        <span class="nb">super</span><span class="p">(</span><span class="n">KLDivergenceLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-10"></a>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-11"></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-12"></a>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-13"></a>        <span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span> <span class="o">=</span> <span class="n">inputs</span>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-14"></a>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-15"></a>        <span class="n">kl_batch</span> <span class="o">=</span> <span class="o">-</span> <span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">log_var</span> <span class="o">-</span>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-16"></a>                                <span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span> <span class="o">-</span>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-17"></a>                                <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_var</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-18"></a>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-19"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">kl_batch</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-20"></a>
<a name="rest_code_d4f10f7b8d8942139ac55739d158c82b-21"></a>        <span class="k">return</span> <span class="n">inputs</span>
</pre>
<pre class="code python"><a name="rest_code_def7466887d54102902a96e2b7921785-1"></a><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">KLDivergenceLayer</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">])</span>
</pre>
<p>by itself, it will learn to ignore the input and map all outputs to 0.
It is only when we tack on the decoder that the reconstruction likelihood
is introduced. Only then will we reconcile the likelihood / observed data with
our prior to form the posterior over latent codes.</p>
<p>At this stage we could specify
<tt class="docutils literal">prob_encoder = Model(inputs=x, <span class="pre">outputs=[z_mu,</span> z_sigma])</tt>
and compile it with something like
<tt class="docutils literal"><span class="pre">prob_encoder.compile(optimizer='rmsprop`,</span> loss=None)</tt>.
When we fit it, it would trivially map all inputs to 0 and 1, thus learning the
prior distribution.</p>
<p>inputs mu and log_var are of shape (batch_size, latent_dim)
the loss we add should be scalar. this is unlike loss
function specified in model compile which should returns
loss vector of shape (batch_size,) since it requires
loss for each datapoint in the batch for sample
weighting.</p>
<div class="figure align-center">
<object data="../../images/vae/encoder_full.svg" style="height: 500px;" type="image/svg+xml">
../../images/vae/encoder_full.svg</object>
<p class="caption">Full encoder architecture, including auxiliary KL divergence layer.</p>
</div>
</div>
</div>
<div class="section" id="putting-it-all-together">
<h3>Putting it all together</h3>
<pre class="code python"><a name="rest_code_1c5a85b528844896b91ce5bd40f07c8b-1"></a><span class="n">x</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,))</span>
<a name="rest_code_1c5a85b528844896b91ce5bd40f07c8b-2"></a><span class="n">h</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<a name="rest_code_1c5a85b528844896b91ce5bd40f07c8b-3"></a>
<a name="rest_code_1c5a85b528844896b91ce5bd40f07c8b-4"></a><span class="n">z_mu</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_1c5a85b528844896b91ce5bd40f07c8b-5"></a><span class="n">z_log_var</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_1c5a85b528844896b91ce5bd40f07c8b-6"></a>
<a name="rest_code_1c5a85b528844896b91ce5bd40f07c8b-7"></a><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">KLDivergenceLayer</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">])</span>
<a name="rest_code_1c5a85b528844896b91ce5bd40f07c8b-8"></a><span class="n">z_sigma</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="o">*</span><span class="n">t</span><span class="p">))(</span><span class="n">z_log_var</span><span class="p">)</span>
<a name="rest_code_1c5a85b528844896b91ce5bd40f07c8b-9"></a>
<a name="rest_code_1c5a85b528844896b91ce5bd40f07c8b-10"></a><span class="n">eps</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">K</span><span class="o">.</span><span class="n">random_normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">latent_dim</span><span class="p">)))</span>
<a name="rest_code_1c5a85b528844896b91ce5bd40f07c8b-11"></a><span class="n">z_eps</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">()([</span><span class="n">z_sigma</span><span class="p">,</span> <span class="n">eps</span><span class="p">])</span>
<a name="rest_code_1c5a85b528844896b91ce5bd40f07c8b-12"></a><span class="n">z</span> <span class="o">=</span> <span class="n">Add</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_eps</span><span class="p">])</span>
<a name="rest_code_1c5a85b528844896b91ce5bd40f07c8b-13"></a>
<a name="rest_code_1c5a85b528844896b91ce5bd40f07c8b-14"></a><span class="n">decoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
<a name="rest_code_1c5a85b528844896b91ce5bd40f07c8b-15"></a>    <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
<a name="rest_code_1c5a85b528844896b91ce5bd40f07c8b-16"></a>    <span class="n">Dense</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">)</span>
<a name="rest_code_1c5a85b528844896b91ce5bd40f07c8b-17"></a><span class="p">])</span>
<a name="rest_code_1c5a85b528844896b91ce5bd40f07c8b-18"></a>
<a name="rest_code_1c5a85b528844896b91ce5bd40f07c8b-19"></a><span class="n">x_mean</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre>
<pre class="code python"><a name="rest_code_44c5a96ae5d045e287d340ee8b6f0a5e-1"></a><span class="n">vae</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">eps</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">x_mean</span><span class="p">)</span>
<a name="rest_code_44c5a96ae5d045e287d340ee8b6f0a5e-2"></a><span class="n">vae</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'rmsprop'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">nll</span><span class="p">)</span>
</pre>
<div class="figure align-center">
<object data="../../images/vae/vae_full_shapes.svg" style="height: 500px;" type="image/svg+xml">
../../images/vae/vae_full_shapes.svg</object>
<p class="caption">Variational autoencoder architecture.</p>
</div>
<p>The point of this tutorial is to illustrate the general framework for performing
amortized variational inference using Keras, treating the inference network
(approximate posterior) and the generative network (likelihood) as black-boxes.
What we've used for the encoder and decoder each with a single hidden
full-connected layer is perhaps the minimal viable architecture.
In the examples directory, Keras provides a more sophisticated variational
autoencoder with deconvolutional layers. The architecture definitions can be
trivially copy-pasted here without need to modify anything else.</p>
</div>
</div>
<div class="section" id="model-fitting">
<h2>Model fitting</h2>
<p>We load the training data as usual. Now the <tt class="docutils literal">vae</tt> is explicitly specified with
random noise source as an auxiliary input. This allows to easily control the
base distribution <span class="math">\(p(\mathbf{\epsilon})\)</span> and also how we draw Monte Carlo
samples of <span class="math">\(\mathbf{z}\)</span> for each datapoint <span class="math">\(\mathbf{x}\)</span>. Usually
we just stick with a simple isotropic Gaussian distribution and draw a different
MC sample for each datapoint.</p>
<pre class="code python"><a name="rest_code_153e1df12bff4a5f8cb62f49cb30d1bb-1"></a><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<a name="rest_code_153e1df12bff4a5f8cb62f49cb30d1bb-2"></a><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">original_dim</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
<a name="rest_code_153e1df12bff4a5f8cb62f49cb30d1bb-3"></a><span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">original_dim</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
</pre>
<p>Model fitting feels less intuitive. The <tt class="docutils literal">vae</tt> is compiled with <tt class="docutils literal">loss=None</tt>
explicitly specified which raises a warning. When fit is called, the targets
argument is left unspecified, and the reconstruction loss is optimized through
the <cite>CustomLayer</cite>. This mapping from mathematical problem formulation to code
implementation appears more natural and straightforward. It's easy to understand
at a glance from our call to the <tt class="docutils literal">fit</tt> method that we're training a
probabilistic auto-encoder.</p>
<pre class="code python"><a name="rest_code_e7e156777ef34380a4cf6c5f6355e4de-1"></a><span class="n">vae</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span>
<a name="rest_code_e7e156777ef34380a4cf6c5f6355e4de-2"></a>        <span class="n">x_train</span><span class="p">,</span>
<a name="rest_code_e7e156777ef34380a4cf6c5f6355e4de-3"></a>        <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<a name="rest_code_e7e156777ef34380a4cf6c5f6355e4de-4"></a>        <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
<a name="rest_code_e7e156777ef34380a4cf6c5f6355e4de-5"></a>        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<a name="rest_code_e7e156777ef34380a4cf6c5f6355e4de-6"></a>        <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">x_test</span><span class="p">))</span>
</pre>
<p>Personally, I prefer this view since the all sources of stochasticity emanate
from the inputs to the model.</p>
<div class="section" id="loss-nelbo-convergence">
<h3>Loss (NELBO) Convergence</h3>
<pre class="code python"><a name="rest_code_95c9231c58414e4a8108a9d3e99632c2-1"></a><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<a name="rest_code_95c9231c58414e4a8108a9d3e99632c2-2"></a>
<a name="rest_code_95c9231c58414e4a8108a9d3e99632c2-3"></a><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">hist</span><span class="o">.</span><span class="n">history</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<a name="rest_code_95c9231c58414e4a8108a9d3e99632c2-4"></a>
<a name="rest_code_95c9231c58414e4a8108a9d3e99632c2-5"></a><span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'NELBO'</span><span class="p">)</span>
<a name="rest_code_95c9231c58414e4a8108a9d3e99632c2-6"></a><span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'# epochs'</span><span class="p">)</span>
<a name="rest_code_95c9231c58414e4a8108a9d3e99632c2-7"></a>
<a name="rest_code_95c9231c58414e4a8108a9d3e99632c2-8"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>
<div class="figure align-center">
<object data="../../images/vae/nelbo.svg" style="width: 500px;" type="image/svg+xml">
../../images/vae/nelbo.svg</object>
</div>
</div>
</div>
<div class="section" id="model-evaluation">
<h2>Model evaluation</h2>
<pre class="code python"><a name="rest_code_d29cad68f30b4568a4f69d229d9e795a-1"></a><span class="n">encoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z_mu</span><span class="p">)</span>
<a name="rest_code_d29cad68f30b4568a4f69d229d9e795a-2"></a>
<a name="rest_code_d29cad68f30b4568a4f69d229d9e795a-3"></a><span class="c1"># display a 2D plot of the digit classes in the latent space</span>
<a name="rest_code_d29cad68f30b4568a4f69d229d9e795a-4"></a><span class="n">z_test</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<a name="rest_code_d29cad68f30b4568a4f69d229d9e795a-5"></a><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<a name="rest_code_d29cad68f30b4568a4f69d229d9e795a-6"></a><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">z_test</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z_test</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span>
<a name="rest_code_d29cad68f30b4568a4f69d229d9e795a-7"></a>            <span class="n">alpha</span><span class="o">=.</span><span class="mi">4</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">3</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'viridis'</span><span class="p">)</span>
<a name="rest_code_d29cad68f30b4568a4f69d229d9e795a-8"></a><span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<a name="rest_code_d29cad68f30b4568a4f69d229d9e795a-9"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>
<div class="figure align-center">
<img alt="../../images/vae/result_latent_space.png" src="../../images/vae/result_latent_space.png" style="height: 500px;">
</div>
<pre class="code python"><a name="rest_code_8f0e9fd2e5bb413b9475633d3f6c0606-1"></a><span class="c1"># display a 2D manifold of the digits</span>
<a name="rest_code_8f0e9fd2e5bb413b9475633d3f6c0606-2"></a><span class="n">n</span> <span class="o">=</span> <span class="mi">15</span>  <span class="c1"># figure with 15x15 digits</span>
<a name="rest_code_8f0e9fd2e5bb413b9475633d3f6c0606-3"></a><span class="n">digit_size</span> <span class="o">=</span> <span class="mi">28</span>
<a name="rest_code_8f0e9fd2e5bb413b9475633d3f6c0606-4"></a>
<a name="rest_code_8f0e9fd2e5bb413b9475633d3f6c0606-5"></a><span class="c1"># linearly spaced coordinates on the unit square were transformed</span>
<a name="rest_code_8f0e9fd2e5bb413b9475633d3f6c0606-6"></a><span class="c1"># through the inverse CDF (ppf) of the Gaussian to produce values</span>
<a name="rest_code_8f0e9fd2e5bb413b9475633d3f6c0606-7"></a><span class="c1"># of the latent variables z, since the prior of the latent space</span>
<a name="rest_code_8f0e9fd2e5bb413b9475633d3f6c0606-8"></a><span class="c1"># is Gaussian</span>
<a name="rest_code_8f0e9fd2e5bb413b9475633d3f6c0606-9"></a><span class="n">u_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span>
<a name="rest_code_8f0e9fd2e5bb413b9475633d3f6c0606-10"></a>                               <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">n</span><span class="p">)))</span>
<a name="rest_code_8f0e9fd2e5bb413b9475633d3f6c0606-11"></a><span class="n">z_grid</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">u_grid</span><span class="p">)</span>
<a name="rest_code_8f0e9fd2e5bb413b9475633d3f6c0606-12"></a><span class="n">x_decoded</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">z_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<a name="rest_code_8f0e9fd2e5bb413b9475633d3f6c0606-13"></a><span class="n">x_decoded</span> <span class="o">=</span> <span class="n">x_decoded</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">digit_size</span><span class="p">,</span> <span class="n">digit_size</span><span class="p">)</span>
<a name="rest_code_8f0e9fd2e5bb413b9475633d3f6c0606-14"></a>
<a name="rest_code_8f0e9fd2e5bb413b9475633d3f6c0606-15"></a><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<a name="rest_code_8f0e9fd2e5bb413b9475633d3f6c0606-16"></a><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">x_decoded</span><span class="p">))),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
<a name="rest_code_8f0e9fd2e5bb413b9475633d3f6c0606-17"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>
<div class="figure align-center">
<img alt="../../images/vae/result_manifold.png" src="../../images/vae/result_manifold.png" style="height: 600px;">
</div>
</div>
<div class="section" id="recap">
<h2>Recap</h2>
<ul class="simple">
<li>Demonstration of Sequential and functional Model API</li>
<li>Custom auxiliary layers that augments the model loss</li>
<li>Fixing input to source of stochasticity</li>
<li>Reparameterization using Merge layers</li>
</ul>
</div>
<div class="section" id="what-s-next">
<h2>What's next</h2>
<p>Normalizing flows</p>
<p>We illustrate how to employ the simple Gumbel-Softmax reparameterization to
build a Categorical VAE with discrete latent variables.</p>
<p>We can easily extend <tt class="docutils literal">KLDivergenceLayer</tt> to use an auxiliary density ratio
estimator function, instead of evaluating the KL divergence in the closed-form
expression above.
This relaxes the requirement on approximate posterior
<span class="math">\(q(\mathbf{z}|\mathbf{x})\)</span> (and incidentally, also prior
<span class="math">\(p(\mathbf{z})\)</span>) to yield tractable densities, at the cost of maximizing
a cruder estimate of the ELBO.
This is known as Adversarial Variational Bayes <a class="footnote-reference" href="#mescheder2017" id="id14">[9]</a>, and is an
important line of recent research that extends the applicability of variational
inference to arbitrarily expressive implicit probabilistic models <a class="footnote-reference" href="#tran2017" id="id15">[10]</a>.</p>
</div>
<div class="section" id="footnotes">
<h2>Footnotes</h2>
<table class="docutils footnote" frame="void" id="id16" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id6">[*]</a></td>
<td>
<p class="first">For a complete treatment of variational autoencoders, and variational
inference in general, I highly recommend:</p>
<ul class="last simple">
<li>Jaan Altosaar's blog post, <a class="reference external" href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/">What is a variational autoencoder?</a>.</li>
<li>Diederik P. Kingma's PhD Thesis,
<a class="reference external" href="https://www.dropbox.com/s/v6ua3d9yt44vgb3/cover_and_thesis.pdf?dl=1">Variational Inference and Deep Learning: A New Synthesis</a>.</li>
</ul>
</td>
</tr></tbody>
</table>
</div>
<div class="section" id="references">
<h2>References</h2>
<table class="docutils footnote" frame="void" id="kingma2014" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id1">[1]</a></td>
<td>D. P. Kingma and M. Welling,
"Auto-Encoding Variational Bayes,"
in Proceedings of the 2nd International Conference on Learning
Representations (ICLR), 2014.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="inference1" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id2">[2]</a></td>
<td><a class="reference external" href="http://edwardlib.org/tutorials/inference-networks">Edward tutorial on Inference Networks</a></td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="inference2" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id3">[3]</a></td>
<td>Section "Recognition models and amortised inference" in
<a class="reference external" href="http://blog.shakirm.com/2015/01/variational-inference-tricks-of-the-trade/">Shakir's blog post</a>.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="dayan1995" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id4">[4]</a></td>
<td>Dayan, P., Hinton, G. E., Neal, R. M., &amp; Zemel, R. S. (1995).
The Helmholtz machine. Neural Computation, 7(5), 889904.
<a class="reference external" href="http://doi.org/10.1162/neco.1995.7.5.889">http://doi.org/10.1162/neco.1995.7.5.889</a>
</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="rezende2014" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label">[5]</td>
<td>
<em>(<a class="fn-backref" href="#id5">1</a>, <a class="fn-backref" href="#id12">2</a>, <a class="fn-backref" href="#id13">3</a>)</em> Rezende, D. J., Mohamed, S., &amp; Wierstra, D. (2014).
"Stochastic backpropagation and approximate inference in deep generative models,"
in Proceedings of The 31st International Conference on Machine Learning, 2014,
(Vol. 32, pp. 12781286). Bejing, China: PMLR. <a class="reference external" href="http://doi.org/10.1051/0004-6361/201527329">http://doi.org/10.1051/0004-6361/201527329</a>
</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="rezende2015" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id7">[6]</a></td>
<td>D. Rezende and S. Mohamed,
"Variational Inference with Normalizing Flows,"
in Proceedings of the 32nd International Conference on Machine Learning, 2015,
vol. 37, pp. 15301538.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="burda2015" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id8">[7]</a></td>
<td>Y. Burda, R. Grosse, and R. Salakhutdinov,
"Importance Weighted Autoencoders,"
in Proceedings of the 3rd International Conference on Learning
Representations (ICLR), 2015.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="jang2016" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id9">[8]</a></td>
<td>E. Jang, S. Gu, and B. Poole,
"Categorical Reparameterization with Gumbel-Softmax," Nov. 2016.
in Proceedings of the 5th International Conference on Learning
Representations (ICLR), 2017.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="mescheder2017" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label">[9]</td>
<td>
<em>(<a class="fn-backref" href="#id10">1</a>, <a class="fn-backref" href="#id14">2</a>)</em> L. Mescheder, S. Nowozin, and A. Geiger,
"Adversarial Variational Bayes: Unifying Variational Autoencoders and
Generative Adversarial Networks,"
in Proceedings of the 34th International Conference on Machine Learning, 2017,
vol. 70, pp. 23912400.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="tran2017" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label">[10]</td>
<td>
<em>(<a class="fn-backref" href="#id11">1</a>, <a class="fn-backref" href="#id15">2</a>)</em> D. Tran, R. Ranganath, and D. Blei,
"Hierarchical Implicit Models and Likelihood-Free Variational Inference,"
<em>to appear in</em> Advances in Neural Information Processing Systems 31, 2017.</td>
</tr></tbody>
</table>
</div>
<div class="section" id="appendix">
<h2>Appendix</h2>
<p>Below, you can find:</p>
<ul class="simple">
<li>The <a class="reference external" href="../../listings/vae/variational_autoencoder.ipynb.html">accompanying Jupyter Notebook</a> used to generate the diagrams and plots
in this post.</li>
<li>The above snippets combined in a single executable Python file:</li>
</ul>
<p><a class="reference external" href="../../listings/vae/variational_autoencoder_improved.py.html">vae/variational_autoencoder_improved.py</a>  <a class="reference external" href="../../listings/vae/variational_autoencoder_improved.py">(Source)</a></p>
<pre class="code python"><a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-1"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-2"></a><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-3"></a><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-4"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-5"></a><span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-6"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-7"></a><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">,</span> <span class="n">Layer</span><span class="p">,</span> <span class="n">Add</span><span class="p">,</span> <span class="n">Multiply</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-8"></a><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">Sequential</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-9"></a><span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-10"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-11"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-12"></a><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-13"></a><span class="n">original_dim</span> <span class="o">=</span> <span class="mi">784</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-14"></a><span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">2</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-15"></a><span class="n">intermediate_dim</span> <span class="o">=</span> <span class="mi">256</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-16"></a><span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-17"></a><span class="n">epsilon_std</span> <span class="o">=</span> <span class="mf">1.0</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-18"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-19"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-20"></a><span class="k">def</span> <span class="nf">nll</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-21"></a>    <span class="sd">""" Bernoulli negative log likelihood. """</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-22"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-23"></a>    <span class="c1"># keras.losses.binary_crossentropy gives the mean</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-24"></a>    <span class="c1"># over the last axis. We require the sum.</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-25"></a>    <span class="k">return</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">binary_crossentropy</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-26"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-27"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-28"></a><span class="k">class</span> <span class="nc">KLDivergenceLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-29"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-30"></a>    <span class="sd">""" Identity transform layer that adds KL divergence</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-31"></a><span class="sd">    to the final model loss.</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-32"></a><span class="sd">    """</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-33"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-34"></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-35"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">is_placeholder</span> <span class="o">=</span> <span class="bp">True</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-36"></a>        <span class="nb">super</span><span class="p">(</span><span class="n">KLDivergenceLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-37"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-38"></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-39"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-40"></a>        <span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span> <span class="o">=</span> <span class="n">inputs</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-41"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-42"></a>        <span class="n">kl_batch</span> <span class="o">=</span> <span class="o">-</span> <span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">log_var</span> <span class="o">-</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-43"></a>                                <span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span> <span class="o">-</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-44"></a>                                <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_var</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-45"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-46"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">kl_batch</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-47"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-48"></a>        <span class="k">return</span> <span class="n">inputs</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-49"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-50"></a><span class="n">x</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,))</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-51"></a><span class="n">h</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-52"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-53"></a><span class="n">z_mu</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-54"></a><span class="n">z_log_var</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-55"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-56"></a><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">KLDivergenceLayer</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">])</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-57"></a><span class="n">z_sigma</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="o">*</span><span class="n">t</span><span class="p">))(</span><span class="n">z_log_var</span><span class="p">)</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-58"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-59"></a><span class="n">eps</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">K</span><span class="o">.</span><span class="n">random_normal</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="n">epsilon_std</span><span class="p">,</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-60"></a>                                   <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">latent_dim</span><span class="p">)))</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-61"></a><span class="n">z_eps</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">()([</span><span class="n">z_sigma</span><span class="p">,</span> <span class="n">eps</span><span class="p">])</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-62"></a><span class="n">z</span> <span class="o">=</span> <span class="n">Add</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_eps</span><span class="p">])</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-63"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-64"></a><span class="n">decoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-65"></a>    <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-66"></a>    <span class="n">Dense</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">)</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-67"></a><span class="p">])</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-68"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-69"></a><span class="n">x_mean</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-70"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-71"></a><span class="n">vae</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">eps</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">x_mean</span><span class="p">)</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-72"></a><span class="n">vae</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'rmsprop'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">nll</span><span class="p">)</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-73"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-74"></a><span class="c1"># train the VAE on MNIST digits</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-75"></a><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-76"></a><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">original_dim</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-77"></a><span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">original_dim</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-78"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-79"></a><span class="n">vae</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-80"></a>        <span class="n">x_train</span><span class="p">,</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-81"></a>        <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-82"></a>        <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-83"></a>        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-84"></a>        <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">x_test</span><span class="p">))</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-85"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-86"></a><span class="n">encoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z_mu</span><span class="p">)</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-87"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-88"></a><span class="c1"># display a 2D plot of the digit classes in the latent space</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-89"></a><span class="n">z_test</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-90"></a><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-91"></a><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">z_test</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z_test</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-92"></a>            <span class="n">alpha</span><span class="o">=.</span><span class="mi">4</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">3</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'viridis'</span><span class="p">)</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-93"></a><span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-94"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-95"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-96"></a><span class="c1"># display a 2D manifold of the digits</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-97"></a><span class="n">n</span> <span class="o">=</span> <span class="mi">15</span>  <span class="c1"># figure with 15x15 digits</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-98"></a><span class="n">digit_size</span> <span class="o">=</span> <span class="mi">28</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-99"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-100"></a><span class="c1"># linearly spaced coordinates on the unit square were transformed</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-101"></a><span class="c1"># through the inverse CDF (ppf) of the Gaussian to produce values</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-102"></a><span class="c1"># of the latent variables z, since the prior of the latent space</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-103"></a><span class="c1"># is Gaussian</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-104"></a><span class="n">u_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-105"></a>                               <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">n</span><span class="p">)))</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-106"></a><span class="n">z_grid</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">u_grid</span><span class="p">)</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-107"></a><span class="n">x_decoded</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">z_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-108"></a><span class="n">x_decoded</span> <span class="o">=</span> <span class="n">x_decoded</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">digit_size</span><span class="p">,</span> <span class="n">digit_size</span><span class="p">)</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-109"></a>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-110"></a><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-111"></a><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">x_decoded</span><span class="p">))),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
<a name="rest_code_e1a1eadffa2b496da555f17e5b477c10-112"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../tags/deep-learning/" rel="tag">deep learning</a></li>
            <li><a class="tag p-category" href="../../tags/keras/" rel="tag">keras</a></li>
            <li><a class="tag p-category" href="../../tags/python/" rel="tag">python</a></li>
            <li><a class="tag p-category" href="../../tags/representation-learning/" rel="tag">representation learning</a></li>
            <li><a class="tag p-category" href="../../tags/tensorflow/" rel="tag">tensorflow</a></li>
            <li><a class="tag p-category" href="../../tags/unsupervised-learning/" rel="tag">unsupervised learning</a></li>
            <li><a class="tag p-category" href="../../tags/variational-autoencoder/" rel="tag">variational autoencoder</a></li>
            <li><a class="tag p-category" href="../../tags/variational-inference/" rel="tag">variational inference</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../../notes/working-with-samples-of-distributions-over-convolutional-kernels/" rel="prev" title="Working with Samples of Distributions over Convolutional Kernels">Previous post</a>
            </li>
            <li class="next">
                <a href="../../notes/working-with-pandas-multiindex-dataframes-reading-and-writing-to-csv-and-hdf5/" rel="next" title="Working with Pandas MultiIndex Dataframes: Reading and Writing to CSV and HDF5">Next post</a>
            </li>
        </ul></nav></aside><section class="comments hidden-print"><h2>Comments</h2>
                        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="ltiao",
            disqus_url="http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/",
        disqus_title="Implementing Variational Autoencoders in Keras: Beyond the Quickstart Tutorial",
        disqus_identifier="cache/content/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha256-SDRP1VVYu+tgAGKhddBSl5+ezofHKZeI+OzxakbIe/Y=" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></article><script>var disqus_shortname="ltiao";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>
        <!--End of body content-->
      </div>
    </div>

    <footer id="footer" class="footer">
        
Contents  2017
<a href="mailto:louistiao@me.com">Louis Tiao</a> - Powered by
<a href="https://getnikola.com" rel="nofollow">Nikola</a>


<span class="pull-right">

  <a class="twitter-follow-button" href="https://twitter.com/louistiao" data-show-count="false" data-show-screen-name="false">
  Follow @louistiao
  </a>

  <a class="github-button" href="https://github.com/ltiao" aria-label="Follow @ltiao on GitHub" data-show-count="false">
  Follow @ltiao
  </a>

  <a href="https://ko-fi.com/A3476EX">
    <object type="image/svg+xml" style="pointer-events: none;" data="https://img.shields.io/badge/Support--yellow.svg?style=social"></object>
  </a>

</span>


            
    </footer>
</div> <!-- /container -->

            <script src="../../assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><!-- Google Analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-43722566-1', 'auto');
  ga('send', 'pageview');

</script><!-- GitHub Buttons --><script async defer src="https://buttons.github.io/buttons.js"></script><!-- Twitter Widgets --><script>window.twttr = (function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0],
    t = window.twttr || {};
  if (d.getElementById(id)) return t;
  js = d.createElement(s);
  js.id = id;
  js.src = "https://platform.twitter.com/widgets.js";
  fjs.parentNode.insertBefore(js, fjs);

  t._e = [];
  t.ready = function(f) {
    t._e.push(f);
  };

  return t;
}(document, "script", "twitter-wjs"));</script>
</body>
</html>
